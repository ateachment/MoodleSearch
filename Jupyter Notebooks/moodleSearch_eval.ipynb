{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "moodleSearch_eval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOoSDPeC/UeutHLUQtVNd2h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ateachment/MoodleSearch/blob/main/moodleSearch_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatisierung deutscher Sprache: \n",
        "https://nickyreinert.de/blog/2020/12/09/einfuehrung-in-stemming-und-lemmatisierung-deutscher-texte-mit-python/\n",
        "\n",
        "Installation von HanoverTagger (wird :"
      ],
      "metadata": {
        "id": "_eOGZGBMo1RR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install HanTa # install lemmatizer of german language"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C4I2-U9oWOh",
        "outputId": "d407814c-c28a-43ca-9769-8bf3db7de93a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: HanTa in /usr/local/lib/python3.7/dist-packages (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vy0yCK5XUOqg",
        "outputId": "573bd949-f83d-4a70-b4d8-0f89cacdcf70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import bs4\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from HanTa import HanoverTagger as ht\n",
        "hannover = ht.HanoverTagger('morphmodel_ger.pgz')\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity # We will use this later to decide how similar two sentences are"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop = nltk.corpus.stopwords.words('german')\n",
        "# Add a few more stop words we would like to remove here\n",
        "stop.append('daher')\n",
        "stop.append('vieler')\n",
        "stop.append('vielen')\n",
        "stop.append('usw')\n",
        "stop.append('bzw')\n",
        "stop.append('etc')\n",
        "stop.append('d.h.')\n",
        "stop.append('u.a')\n",
        "stop.append('z.b')\n",
        "stop.append('--')\n",
        "stop.append('-')\n",
        "stop.append('``')\n",
        "stop.append(\"''\")\n",
        "stop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yvhqsb-kuAAT",
        "outputId": "29332e2c-3a48-4a93-e91e-70f3cf1bc236"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aber',\n",
              " 'alle',\n",
              " 'allem',\n",
              " 'allen',\n",
              " 'aller',\n",
              " 'alles',\n",
              " 'als',\n",
              " 'also',\n",
              " 'am',\n",
              " 'an',\n",
              " 'ander',\n",
              " 'andere',\n",
              " 'anderem',\n",
              " 'anderen',\n",
              " 'anderer',\n",
              " 'anderes',\n",
              " 'anderm',\n",
              " 'andern',\n",
              " 'anderr',\n",
              " 'anders',\n",
              " 'auch',\n",
              " 'auf',\n",
              " 'aus',\n",
              " 'bei',\n",
              " 'bin',\n",
              " 'bis',\n",
              " 'bist',\n",
              " 'da',\n",
              " 'damit',\n",
              " 'dann',\n",
              " 'der',\n",
              " 'den',\n",
              " 'des',\n",
              " 'dem',\n",
              " 'die',\n",
              " 'das',\n",
              " 'dass',\n",
              " 'daß',\n",
              " 'derselbe',\n",
              " 'derselben',\n",
              " 'denselben',\n",
              " 'desselben',\n",
              " 'demselben',\n",
              " 'dieselbe',\n",
              " 'dieselben',\n",
              " 'dasselbe',\n",
              " 'dazu',\n",
              " 'dein',\n",
              " 'deine',\n",
              " 'deinem',\n",
              " 'deinen',\n",
              " 'deiner',\n",
              " 'deines',\n",
              " 'denn',\n",
              " 'derer',\n",
              " 'dessen',\n",
              " 'dich',\n",
              " 'dir',\n",
              " 'du',\n",
              " 'dies',\n",
              " 'diese',\n",
              " 'diesem',\n",
              " 'diesen',\n",
              " 'dieser',\n",
              " 'dieses',\n",
              " 'doch',\n",
              " 'dort',\n",
              " 'durch',\n",
              " 'ein',\n",
              " 'eine',\n",
              " 'einem',\n",
              " 'einen',\n",
              " 'einer',\n",
              " 'eines',\n",
              " 'einig',\n",
              " 'einige',\n",
              " 'einigem',\n",
              " 'einigen',\n",
              " 'einiger',\n",
              " 'einiges',\n",
              " 'einmal',\n",
              " 'er',\n",
              " 'ihn',\n",
              " 'ihm',\n",
              " 'es',\n",
              " 'etwas',\n",
              " 'euer',\n",
              " 'eure',\n",
              " 'eurem',\n",
              " 'euren',\n",
              " 'eurer',\n",
              " 'eures',\n",
              " 'für',\n",
              " 'gegen',\n",
              " 'gewesen',\n",
              " 'hab',\n",
              " 'habe',\n",
              " 'haben',\n",
              " 'hat',\n",
              " 'hatte',\n",
              " 'hatten',\n",
              " 'hier',\n",
              " 'hin',\n",
              " 'hinter',\n",
              " 'ich',\n",
              " 'mich',\n",
              " 'mir',\n",
              " 'ihr',\n",
              " 'ihre',\n",
              " 'ihrem',\n",
              " 'ihren',\n",
              " 'ihrer',\n",
              " 'ihres',\n",
              " 'euch',\n",
              " 'im',\n",
              " 'in',\n",
              " 'indem',\n",
              " 'ins',\n",
              " 'ist',\n",
              " 'jede',\n",
              " 'jedem',\n",
              " 'jeden',\n",
              " 'jeder',\n",
              " 'jedes',\n",
              " 'jene',\n",
              " 'jenem',\n",
              " 'jenen',\n",
              " 'jener',\n",
              " 'jenes',\n",
              " 'jetzt',\n",
              " 'kann',\n",
              " 'kein',\n",
              " 'keine',\n",
              " 'keinem',\n",
              " 'keinen',\n",
              " 'keiner',\n",
              " 'keines',\n",
              " 'können',\n",
              " 'könnte',\n",
              " 'machen',\n",
              " 'man',\n",
              " 'manche',\n",
              " 'manchem',\n",
              " 'manchen',\n",
              " 'mancher',\n",
              " 'manches',\n",
              " 'mein',\n",
              " 'meine',\n",
              " 'meinem',\n",
              " 'meinen',\n",
              " 'meiner',\n",
              " 'meines',\n",
              " 'mit',\n",
              " 'muss',\n",
              " 'musste',\n",
              " 'nach',\n",
              " 'nicht',\n",
              " 'nichts',\n",
              " 'noch',\n",
              " 'nun',\n",
              " 'nur',\n",
              " 'ob',\n",
              " 'oder',\n",
              " 'ohne',\n",
              " 'sehr',\n",
              " 'sein',\n",
              " 'seine',\n",
              " 'seinem',\n",
              " 'seinen',\n",
              " 'seiner',\n",
              " 'seines',\n",
              " 'selbst',\n",
              " 'sich',\n",
              " 'sie',\n",
              " 'ihnen',\n",
              " 'sind',\n",
              " 'so',\n",
              " 'solche',\n",
              " 'solchem',\n",
              " 'solchen',\n",
              " 'solcher',\n",
              " 'solches',\n",
              " 'soll',\n",
              " 'sollte',\n",
              " 'sondern',\n",
              " 'sonst',\n",
              " 'über',\n",
              " 'um',\n",
              " 'und',\n",
              " 'uns',\n",
              " 'unsere',\n",
              " 'unserem',\n",
              " 'unseren',\n",
              " 'unser',\n",
              " 'unseres',\n",
              " 'unter',\n",
              " 'viel',\n",
              " 'vom',\n",
              " 'von',\n",
              " 'vor',\n",
              " 'während',\n",
              " 'war',\n",
              " 'waren',\n",
              " 'warst',\n",
              " 'was',\n",
              " 'weg',\n",
              " 'weil',\n",
              " 'weiter',\n",
              " 'welche',\n",
              " 'welchem',\n",
              " 'welchen',\n",
              " 'welcher',\n",
              " 'welches',\n",
              " 'wenn',\n",
              " 'werde',\n",
              " 'werden',\n",
              " 'wie',\n",
              " 'wieder',\n",
              " 'will',\n",
              " 'wir',\n",
              " 'wird',\n",
              " 'wirst',\n",
              " 'wo',\n",
              " 'wollen',\n",
              " 'wollte',\n",
              " 'würde',\n",
              " 'würden',\n",
              " 'zu',\n",
              " 'zum',\n",
              " 'zur',\n",
              " 'zwar',\n",
              " 'zwischen',\n",
              " 'daher',\n",
              " 'vieler',\n",
              " 'vielen',\n",
              " 'usw',\n",
              " 'bzw',\n",
              " 'etc',\n",
              " 'd.h.',\n",
              " 'u.a',\n",
              " 'z.b',\n",
              " '--',\n",
              " '-',\n",
              " '``',\n",
              " \"''\"]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Crawling"
      ],
      "metadata": {
        "id": "mYZu4r45HPUK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X_Z48OluFm0",
        "outputId": "2272690b-bf80-426f-d806-009e1122b340"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "base_url = 'https://eick-at.de/moodle/course/view.php?id=19'  # URL of the Moodle Course\n",
        "r = requests.get(base_url)                                              # Http Request \n",
        "r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "gqlP7zDWuFm5"
      },
      "outputs": [],
      "source": [
        "#`r.text` contains the raw HTML returned when we made our GET request earlier. \n",
        "#`'html5lib'` tells BeautifulSoup that it is reading HTML information. \n",
        "soup = bs4.BeautifulSoup(r.text,'html5lib')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "5W7IHfpNuFm9"
      },
      "outputs": [],
      "source": [
        "headers = []                         \n",
        "for url in soup.findAll(\"h3\"):      # find sections\n",
        "    headers.append(url)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "headers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buS--jRbWhaW",
        "outputId": "63f2a642-e9a9-4690-c6df-90acda4b4536"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<h3 class=\"sectionname\" id=\"sectionid-165-title\"><span><a href=\"https://eick-at.de/moodle/course/view.php?id=19#section-0\">Allgemeine Informationen</a></span></h3>,\n",
              " <h3 class=\"sectionname\" id=\"sectionid-166-title\"><span><a href=\"https://eick-at.de/moodle/course/view.php?id=19#section-1\">Einführung</a></span></h3>,\n",
              " <h3 class=\"sectionname\" id=\"sectionid-167-title\"><span><a href=\"https://eick-at.de/moodle/course/view.php?id=19#section-2\">Datenexploration</a></span></h3>,\n",
              " <h3 class=\"sectionname\" id=\"sectionid-168-title\"><span><a href=\"https://eick-at.de/moodle/course/view.php?id=19#section-3\">Entwicklungsumgebung</a></span></h3>,\n",
              " <h3 class=\"sectionname\" id=\"sectionid-169-title\"><span><a href=\"https://eick-at.de/moodle/course/view.php?id=19#section-4\">Quellen</a></span></h3>,\n",
              " <h3 class=\"sectionname\" id=\"sectionid-170-title\"><span><a href=\"https://eick-at.de/moodle/course/view.php?id=19#section-5\">KI-Domainen</a></span></h3>,\n",
              " <h3 class=\"sectionname\" id=\"sectionid-171-title\"><span><a href=\"https://eick-at.de/moodle/course/view.php?id=19#section-6\">Machine Learning</a></span></h3>]"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def processText(txt):\n",
        "  tokenized = nltk.tokenize.word_tokenize(txt)\n",
        "  #print(tokenized)\n",
        "  hannovered = [hannover.analyze(word)[0] for word in tokenized]\n",
        "  #print(hannovered)\n",
        "  processed = [w.lower() for w in hannovered if w not in stop]\n",
        "  #print(processed)\n",
        "  return processed\n"
      ],
      "metadata": {
        "id": "MJWjSV4PWotf"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9I9PI1auFnC",
        "outputId": "7b6029e8-4459-493d-ac20-923646c9f4f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://eick-at.de/moodle/course/view.php?id=19#section-0', ['allgemein', 'information']]\n",
            "['https://eick-at.de/moodle/course/view.php?id=19#section-1', ['einführung']]\n",
            "['https://eick-at.de/moodle/mod/page/view.php?id=1358', ['künstlich', 'intelligenz', 'künstlich', 'intelligenz', 'ki', 'engl', 'artificial', 'intelligence', 'ai', 'bezeichnen', 'teilgebiet', 'informatik', 'automatisierung', 'intelligent', 'verhalten', 'maschinell', 'lernen', 'befassen', 'begriff', 'verwendet', 'obwohl', 'zugrunde', 'liegend', 'definition', 'intelligenz', 'durchaus', 'umstritten', 'ki', 'durchaus', 'traditionell', 'art', 'realisert', 'lange', 'reihe', 'wenn-dann-anweisung', 'einfach', 'kontrollstrukturen', 'bzw', 'regel', 'aneinander', 'reihen', 'expertensystem', 'lassen', 'z.b', 'krankheit', 'anhand', 'symptom', 'diagnostizieren', 'ki', 'lassen', 'gewissen', 'grenze', 'klassisch', 'programmierung', 'erreichen', 'allerdings', 'algorithmen', 'je', 'anforderung', 'kompliziert', 'traditionell', 'programmierungbeim', 'maschinell', 'lernen', 'dagegen', 'lernen', 'maschine', 'regel', 'automatisch', 'sammlung', 'bekannte', 'beispiel', 'trainieren', 'maschinell', 'lernen', 'heute', 'gängig', 'methode', 'künstlich', 'intelligenz', 'erreichen', 'maschinell', 'lernendabei', 'deep', 'learning', 'besonderer', 'form', 'maschinell', 'lernen', 'künstlich', 'neuronal', 'netz', 'knn', 'programmatisch', 'nachbildung', 'vernetzen', 'gehirnzell', 'sog', 'neuronen', 'datum', 'trainieren', 'erwartet', 'ergebnis', 'abgleichen', 'dabei', 'nerven-', 'verbindung', 'neuronen', 'unterschiedlich', 'gewichtet', 'bzw', 'verstärken', 'bias', 'dabei', 'deep', 'knn', 'tief', 'viele', 'hidden', 'lay', 'aufweisen', 'gewiß', 'komplexität', 'aufweisen', 'deep', 'learning', 'teilgebiet', 'maschinell', 'lernen', 'wiederum', 'teilbereich', 'künstlich', 'intelligenz', 'darstellen', 'zusammenhang', 'künstlich', 'intelligenz', 'machine', 'learning', 'deep', 'learning']]\n",
            "['https://eick-at.de/moodle/mod/assign/view.php?id=1359', ['aufgabe', 'anwendungsgebiet', 'künstlich', 'intelligenz', 'anwendungsgebiet', 'künstlich', 'intelligenz', 'kennen']]\n",
            "['https://eick-at.de/moodle/mod/page/view.php?id=1360', ['lösung', 'ki-anwendungsgebiete', 'quelle', 'the', 'difference', 'between', 'artificial', 'intelligence', 'machine', 'learning', 'and', 'deep', 'learning', 'intel', 'communities', '31.12.2021', 'künstlich', 'neuronal', 'netz', 'intelligent', 'heizung', 'anwesenheitsdatum', 'bewohner', 'trainieren', 'heizung', 'verlassen', 'letzter', 'bewohner', 'automatisch', 'abschalten', 'ki', 'vorhersagen', 'wann', 'eintreffen', 'erster', 'bewohner', 'rechnen', 'heizen', 'entsprechend', 'wohnung']]\n",
            "['https://eick-at.de/moodle/mod/page/view.php?id=1361', ['ki', 'spielen', 'probieren', 'folgend', 'ki-spiele', 'online', 'semantrisquick', 'draw', 'rock', 'pap', 'scissorsdiskutiere', 'notieren', 'bestmöglich', 'strategie', 'partnerarbeit']]\n",
            "['https://eick-at.de/moodle/mod/page/view.php?id=1362', ['biologisch', 'künstlich', 'neuron', 'neuronen', 'nervenzell', 'nervensystem', 'bestehen', 'zusammen', 'gliazell', 'menschlich', 'gehirn', 'enthalten', 'dabei', 'ca', '90', 'milliarde', 'nervenzell', 'neuron', 'empfangen', 'signal', 'mehreren', 'gehirnzell', 'erregung', 'unterschiedlich', 'gewichtet', 'aufsummieren', 'lassen', 'bestimmt', 'schwellenpotential', 'überschreiten', 'neuron', '-prinzip', 'seinerseits', 'feuer', 'd.h.', 'neuron', 'erregen', 'zelle', 'denen', 'nervenfasern', 'fest', 'verbunden', 'natürlich', 'neuronen', 'synapse', 'https', '//nl.dreamstime.com/transmissie-van-het-zenuwsignaal-tussen-twee-neuronen-met-axon-en-synaps-close-up-een-chemische-neurale-mededeling-vectordiagram-image153760677', 'bearbeiten', '20.02.2022', 'dabei', 'spielen', 'synapsen', 'wichtig', 'rolle', 'bezeichnen', 'übergangsstelle', 'denen', 'erregung', 'zelle', 'übertragen', 'überwiegend', 'signal', 'chemisch', 'neurotransmitter', 'synaptisch', 'spalt', 'übertragen', 'synapsen', 'anpassungsfähig', 'menge', 'chemisch', 'botenstoff', 'effekt', 'effektivität', 'schnittstelle', 'nutzung', 'verstärken', 'abschwächen', 'vorhanden', 'fehlend', 'training', 'menschlich', 'gehirn', 'befinden', 'schätzen', '1', 'billiarde', 'synapsen', 'nervenverbindung', 'informationstechnisch', 'lassen', 'vernetzen', 'neuronen', 'stark', 'vereinfachen', 'nachbilden', 'künstlich', 'neuron', 'künstlich', 'neuron', 'feueren', 'wesentlich', 'einzeln', 'neuron', 'neuron', 'berechnen', 'gegensatz', 'natürlich', 'neuronal', 'netz', 'laufen', 'aufsummierung', 'eingehend', 'gewichtet', 'erregung', 'neuronen', 'gleichzeitig', 'parallel', 'ab', 'gewichtung', 'verbindung', 'https', '//medium.com/', '@', 'najeebnik21/activation-function-in-deep-learning-587e83d5a681', '17.02.2022', 'genauso', 'künstlich', 'neuronal', 'netz', 'schrittweise', 'trainieren', 'bedeuten', 'lernprozess', 'gewichtung', 'eingehend', 'signal', 'neuron', 'neuron', 'zeitlich', 'nacheinander', 'neu', 'berechnen', 'müssen', 'echt', 'parallelverarbeitung', 'viele', 'rechenkerne', 'beschleunigen', 'sowohl', 'verarbeitungs-', 'trainingsprozess', 'stark', 'graphic', 'processing', 'unit', 'gpu', 'stark', 'vorteil', 'verfügen', 'hundert', 'tausend', 'cor', 'wegen', 'graphisch', 'berechnung', 'durchsatz', 'hoch', 'verarbeitungsgeschwindigkeit', 'einzeln', 'befehl', 'central', 'processing', 'unit', 'cpus', 'optimieren', 'jetson', 'nano', 'nvidia', '128-kern', 'gpu', 'jetson', 'nano', 'resourcensparender', 'einplatinen-rechner', '128-kern', 'gpu', 'ki-projekte', 'cpu-leistung', 'etwa', 'rasperry', 'pi', 'vergleichbar', 'https', '//developer.nvidia.com/embedded/dlc/jetson-nano-system-module-datasheettensor', 'processing', 'unit', 'tpus', 'dagegen', 'anwendungsspezifisch', 'chip', 'asics', 'speziell', 'anforderung', 'maschinell', 'lernen', 'entwerfen', 'parallelverarbeitung', 'viele', 'compute', 'unit', 'unterstützen', 'notwendig', 'algorithmen', 'z.b', 'matrizenrechnung', 'optimieren', 'fließkomma-berechnung', 'realisieren', 'ganz', 'besonders', 'leistungsfähig', 'googeln', 'entwickeln', 'tpus', 'speziell', 'softwaresammlung', 'tensorflow', 'unternehmen', 'entwerfen', 'tpus', 'basis', 'googeln', 'services', 'maschinell', 'lernen', 'einsetzen']]\n",
            "['https://eick-at.de/moodle/course/view.php?id=19#section-2', ['datenexploration']]\n",
            "['https://eick-at.de/moodle/mod/page/view.php?id=1363', ['data-mining', 'begriff', 'data-mining', 'eigentlich', 'missverständlich', 'gehen', 'förderung', 'neu', 'datum', 'to', 'mine', '=', 'graben', 'abbau', 'fördern', 'untersuchung', 'bereits', 'vorhanden', 'groß', 'datenmengen', 'bestimmen', 'zusammenhang', 'trend', 'regelmäßigkeit', 'gesetzmäßigkeit', 'abweichung', 'usw', 'hierfür', 'oft', 'experimentell', 'statistisch', 'methode', 'graphisch', 'aufbereitung', 'eingesetzt.gelegentlich', 'verstehen', 'darunter', 'zusätzlich', 'erfassung', 'z.b', 'mittels', 'web', 'scraping', 'speicherung', 'aufarbeitung', 'datum', 'wissenschaftlich', 'sinn', 'eigentlich', 'vorbereitend', 'prozessschritte', 'sogenannter', 'knowledge', 'discovery', 'databases', 'prozeß', 'data-mining', 'schritt', 'darstellen', 'data', 'mining', 'kdd-prozessdata-mining', 'vielleicht', 'deutsch', 'ehesten', 'datenmustererkennung', '-entdeckung', 'bezeichnen.data-mining', 'entwicklung', 'ki', 'wichtig', 'gehen', 'wesentlich', 'darum', 'herausfinden', 'feature', 'datenmerkmalen', 'künstlich', 'neuronal', 'netz', 'trainieren', 'müssen', 'ende', 'gültig', 'vorhersagen', 'erhalten']]\n",
            "['https://eick-at.de/moodle/mod/assign/view.php?id=1364', ['aufgabe', 'data-mining', 'like-button', 'facebook', 'hören', 'vortrag', 'david', 'kriesels', 'spiegel-mining', 'analysieren', 'diskutieren', 'beschreiben', 'data-mining', 'möglichkeit', 'gefahr', 'hinblick', 'facebook', 'like-button']]\n",
            "['https://eick-at.de/moodle/mod/page/view.php?id=1365', ['lösungsvorschlag', 'data-mining', 'like-button', 'facebook', 'david', 'kriesel', 'umreißen', 'vortrag', 'scenario', 'facebook', 'anhand', 'likes', 'benutzer', 'einstellung', 'vorliebe', 'schließen', 'anhand', 'gespeichert', 'personell', 'datum', 'alter', 'geschlecht', 'allein', 'sicher', 'rückschluß', 'darauf', 'vornehmen', 'aufruf', 'jeweilig', 'facebook-seit', 'unbedingt', 'bild', 'komplett', 'aussagen', 'inhalt', 'fällen', 'allerdings', 'lassen', 'natürlich', 'zusätzlich', 'verweildauer', 'nächster', 'aufruf', 'facebook-seit', 'messen', 'bzw', 'absolut', 'anzahl', 'einzeln', 'facebook-seitenaufrufe', 'speichern', 'umstand', 'bereits', 'weitere', 'nützlich', 'feature', '.der', 'like', 'binäres', 'feature', 'unklar', 'bleiben', 'negativ', 'wert', 'egal', 'interessieren', 'bedeuten', 'positiv', 'wert', 'nutzer', 'aktiv', 'zustimmung', 'inhalt', 'erklären', 'zahl', 'likes', 'weiteres', 'feature', 'auswerten', 'lassen', 'demnach', 'gewissen', 'mindestanzahl', 'likes', 'wahrscheinlich', 'gut', 'prognose', 'abgeben', 'heißen', 'z.b', 'politisch', 'einstellung', 'art', 'bevorzugen', 'ansprache', 'duzen', 'siezen', 'gut', 'vorhersage', 'personalisiert', 'wahlwerbung', 'möglich', 'gehen', 'demokratie', 'natürlich', 'weit', 'manipulation', 'kaufverhalten', 'hinaus', 'müssen', 'entsprechend', 'kritisch', 'beobachtet']]\n",
            "['https://eick-at.de/moodle/mod/page/view.php?id=1366', ['nlp-basiert', 'suchmaschine', 'normalerweise', 'basieren', 'suchen', 'innerhalb', 'webauftritts', 'sql-datenbankabfragen', 'suchworte', 'innerhalb', 'webauftritts', 'einzeln', 'seite', 'zuordnen', 'können.beispiel', 'codeauszug', 'suchen', 'innerhalb', 'wordpress-installation', 'quelle', 'wpdb', ':esc_like', '|', 'method', '|', 'wordpress', 'developer', 'resources', '10.03.2022', 'suchmaschine', 'natürlich', 'zugang', 'datumenbank', 'internet', 'service', 'provider', 'suchworte', 'webseiten', 'abgleichen', 'suchmaschine', 'müssen', 'zwangsläufig', 'webseiten', 'lesen', 'sogenannter', 'internet-robots', 'kurz', 'bot', 'programm', 'selbstständig', 'internetlinks', 'folgen', 'suchmaschinen-anbieter', 'inhalt', 'auslesen', 'übertragen', 'heißen', 'crawler', 'quasi', 'internet', 'kriechen', 'engl', 'to', 'crawl', 'automatisiert', 'auslesen', 'webseiten', 'web', 'scraping', 'kratzen', '=', 'engl', 'to', 'scrape', 'bezeichnet.die', 'bot', 'übertragen', 'webseiteninhalte', 'müssen', 'aufbereiten', 'abspeichern', 'groß', 'suchmaschinen-anbieter', 'bereit', 'text', 'hilfe', 'natural', 'language', 'processing', 'z.b', 'vergleich', 'erfolgen', 'danach', 'verfahren', 'beispielhaft', 'anwenden', 'moodle-kurs', 'https', '//eick-at.de', 'öffentlich', 'spiegeln', 'suchen', 'seite', 'programmieren', 'https', '//github.com/ateachment/moodlesuche/blob/f88365d6eaa65f0a5eb718bc25ef7530f0181629/moodlesuche.ipynbzunächst', 'strartseit', 'kurs', 'aufgerufen', 'r.text', 'enthalten', 'unbearbeitet', 'text', 'seite', 'python-bibliothek', 'beautiful', 'soup', 'html', 'parser', 'html-code', 'startseite', 'kurs', 'analysieren', 'html5lib', 'weisen', 'beautifulsoup', 'dabei', 'html-information', 'auswerten', 'startseite', 'enthalten', 'inhaltsverzeichnis', 'kurs', 'inspektion', 'startseite', 'ergeben', 'abschnitt', 'kurs', 'h3-element', 'überschrift', 'engl', 'heading', '3', 'ordnung', 'gekennzeichnet', '<', 'h3', '>', '-tags', 'parser', 'html-', 'suppe', 'liste', 'headers', 'kopieren', 'listenelement', 'enthalten', 'z.b', 'folgend', 'html-code', 'head', 'schleife', 'durchlaufen', 'links', 'sowie', 'link-texte', 'extrahiert', 'link-texte', 'dabei', 'nlp', 'verarbeiten', 'beides', 'liste', 'data', 'abgelegt.jetzt', 'müssen', 'einzeln', 'jeweilig', 'seite', 'aufgabe', 'links', 'usw', 'abschnitt', 'crawler', 'durchlaufen', 'html-inspektion', 'zeigen', 'links', 'eintrag', 'listenpunkt', 'ungeordneten', 'liste', 'aufführen', 'head', 'zunächst', 'übergeordnet', '<', 'ul', '>', '-tag', 'suchen', 'unordered', 'listen', 'einzeln', '<', 'li', '>', '-tags', 'listen', 'item', 'weit', 'schleife', 'durchlaufen', 'links', 'extern', 'url', 'verweisen', 'extrahiert', 'prinzipiell', 'gleich', 'verfahren', 'seite', 'kurs', 'anwenden', 'parameter', 'txt', 'enthalten', 'bereits', 'html-tags', 'mehr', 'lauten', 'z.b.:1', 'wort-tokenisierung', 'python-bibliothek', 'nltk', 'natural', 'language', 'toolkit', 'trennen', 'text', 'anhand', 'leerzeich', 'satzzeichen', 'wort', 'satzzeichen', 'liste', 'geschrieben:2', 'hanover-lemmatizer', 'deutsch', 'sprache', 'ersetzen', 'satzzeichen', 'reduzieren', 'wort', 'einzahl', 'grund-', 'bzw', 'stammform:3', '+', '4', 'abschließend', 'stop-wörter', 'entfernt', 'usw', 'relevanz', 'besitz', 'außerdem', 'durchgehend', 'kleinschreibung', 'anwenden', 'nächster', 'schritt', 'bestehen', 'darin', 'häufigkeit', 'wort', 'gesamt', 'wortschatz', 'bestimmen', 'ergebnis', 'sehen', 'folgendermaßen', 'hashmap', 'zuordnung', 'schlüssel', 'key', 'z.b', \"'künstlich\", \"'\", 'wert', 'zuordnen', 'z.b', \"'14\", \"'\", 'wort', \"'künstlich\", \"'\", 'kommen', 'genau', '14', 'mal', 'gesamt', 'wortschatz', 'vor.jetzt', 'gesamt', 'wortschatz', 'häufigkeit', 'vorkommen', 'wort', 'absteigend', 'sortieren', 'zweiter', 'parameter', 'begrenzen', 'dabei', 'lang', 'liste', 'datumenmenge', 'folgend', 'verbunden', 'rechenaufwand', 'begrenzen', 'dabei', 'davon', 'ausgehen', 'selten', 'vorkommend', 'wort', 'weniger', 'relevant', 'eher', 'abgeschnitten', 'dürfen', 'häufig', 'vorkommend', 'begriff', 'vorne', 'liste', 'stehen', 'wichtig', 'wort', 'durchaus', 'selten', 'vorkommen', 'verarbeitung', 'ausschließen', 'parameter', 'müssen', 'sorgfältig', 'abhängigkeit', 'rechenleistung', 'bzw', 'verarbeitungsdauer', 'anpassen', 'werden.das', 'ergebnis', 'z.b', 'mathematisch', 'verarbeitung', 'wort', 'direkt', 'möglich', 'müssen', 'erst', 'zahlen', 'umwandeln', 'dafür', 'sogenannter', 'bag', 'of', 'words', 'erstellen', 'liefern', 'folgendes', 'ergebnis', 'wort', 'künstlich', 'intelligenz', 'beiden', 'häufig', 'wort', 'überhaupt', 'maschinell', 'kommen', 'dagegen', 'erst', '10', 'stelle', 'bag', 'of', 'words', 'array', '1000', 'element', 'stellen', 'vektor', '1000-dimensionalen', 'raum', 'dar', 'folgend', 'vektoren', 'miteinander', 'mathematisch', 'vergleichen', 'ähnlichkeit', 'berechnen', 'können.dazu', 'müssen', 'zunächst', 'gescrapte', 'seite', 'bag', 'of', 'words', 'erstellen', 'dafür', '2-dimensionales', 'array', 'bag_o', 'erzeugen', 'zeile', 'bag', 'of', 'words', 'seite', 'enthalten', 'bevor', 'vergleich', 'such-bag', 'of', 'words', 'bag', 'of', 'words', 'seite', 'vornehmen', 'müssen', 'relevanz', 'wort', 'berücksichtigen', 'bag', 'of', 'words', 'abbilden', 'begriff', 'seite', 'vorkommen', 'relevant', 'diejenigen', 'wort', 'einzeln', 'seite', 'vorkommen', 'suchfunktion', 'aussagekräftiger.die', 'gewichtung', 'wort', 'bag', 'of', 'words', 'müssen', 'entsprechend', 'anpassen', 'wort', 'seite', 'vorkommen', 'weniger', 'gewichtet', 'begriff', 'wenigen', 'seite', 'vorkommen', 'stark', 'gewichtet', 'gleichzeitig', 'bleiben', 'allerdings', 'wichtig', 'oft', 'begriff', 'überhaupt', 'seite', 'vorkommt.tf-idf', 'stehen', 'term', 'frequency', 'inverse', 'document', 'frequency', 'of', 'records', 'term', 'umgangssprachlich', 'wort', 'begriff', 'bedeutung', '.die', 'termhäufigkeit', 'tf', 'term', 'frequency', 'beschreiben', 'oft', 'wort', 'dokument', 'seite', 'vorkommen', 'je', 'öfter', 'wort', 'dokument', 'vorkommen', 'groß', 'bedeutung', 'suchen', '\\\\', 'tf', '=', '\\\\frac', '{', 'anzahl\\\\', 'eines\\\\', 'wortes\\\\', 'im\\\\', 'dokument', '}', '{', 'gesamtwortzahl\\\\', 'des\\\\', 'dokument', '}', '\\\\', 'dokumentenhäufigkeit', 'df', 'document', 'frequency', 'besagen', 'dokument', 'term', 'vorhanden', '\\\\', 'df', '=', 'anzahl\\\\', 'der\\\\', 'dokumente\\\\', 'in\\\\', 'denen\\\\', 'ein\\\\', 'bestimmtes\\\\', 'wort\\\\', 'vorkommen', '\\\\', 'relevanz', 'wort', 'umkehren', 'proportional', 'df', 'anzahl', 'dokument', 'denen', 'wort', 'vorkommen', '\\\\', 'relevanz\\\\', 'eines\\\\', 'wortes\\\\', '\\\\sim', '\\\\frac', '{', '1', '}', '{', 'df', '}', '\\\\', 'umkehren', 'proportional', '=', 'engl', 'inverse', 'proportional', 'außerdem', 'dabei', 'relevant', 'wieviele', 'dokument', 'überhaupt', 'geben', 'n', '\\\\', 'relevanz\\\\', 'eines\\\\', 'wortes\\\\', '\\\\sim', 'gesamtzahl\\\\', 'aller\\\\', 'dokumente\\\\', 'n', '\\\\', 'beiden', 'schlussfolgerung', 'ergeben', 'inverse', 'dokumentenhäufigkeit', 'idf', 'inverse', 'document', 'frequency', 'wort', '\\\\', 'relevanz\\\\', 'eines\\\\', 'wortes\\\\', '=', 'idf', '=', '\\\\frac', '{', 'n', '}', '{', 'df', '}', '\\\\', 'kommen', 'wort', 'dokument', 'ergeben', 'wert', '1', 'tatsächlich', 'relevanz', 'bedeutung', 'suchen', '=', '0', 'völlig', 'unbrauchbar', 'logarithmus', 'basis', '2', 'bruch', 'bestimmen', 'gelten', 'log', '1', '=', '0', 'gewichtung', 'gehen', '0', 'wort', 'dokument', 'vorkommen', 'logarithmusfunktion', 'basis', '2', 'https', '//en.wikipedia.org/wiki/binary_logarithm', '12.03.2022', 'müssen', 'jedoch', 'außerdem', 'berücksichtigen', 'oft', 'bestimmt', 'wort', 'dokument', 'vorkommen', 'tf', 'term', 'frequency', 'berechnung', 'relevanz', 'tf', 'idf', 'mulitpliziert', '\\\\', 'tfidf', '=', 'tf', '\\\\cdot', '\\\\frac', '{', 'n', '}', '{', 'df', '}', '\\\\', 'code', 'manuell', 'programmieren', 'python-bibliothek', 'sklearn', 'bieten', 'allerdings', 'fertig', 'algorithmus', 'tfidf', 'sehen', 'folgendermaßen', 'wort', 'wenigen', 'seite', 'vorkommen', 'stark', 'gewichtet.diese', 'datum', 'speichern', 'mal', 'webcrawling', 'nlp', 'ermitteln', 'müssen', 'zeitgesteuerter', 'task', 'tag', 'erledigen', 'webcrawler', 'durchlaufen', 'netz', 'vorzugsweise', 'netzwerkbelastung', 'gering', 'datum', 'kompakt', 'binären', 'format', 'speichern', 'mensch', 'lesbar', 'müssen.das', 'lesen', 'datei', 'erfolgen', 'such-vektor', 'seiten-vektoren', 'vergleichen', 'dafür', 'jeweils', 'kosinus-ähnlichkeit', 'vektoren', 'berechnen', '\\\\', 'co', '\\\\theta', '=', '\\\\frac', '{', '\\\\sigma', '{', 'a_i', '\\\\cdot', 'b_i', '}', '}', '{', '\\\\sqrt', '{', '\\\\sigma', '{', 'a_i^2', '}', '}', '\\\\cdot', '\\\\sqrt', '{', '\\\\sigma', '{', 'b_i^2', '}', '}', '}', '=', '\\\\frac', '{', 'a_1b_1+a_2b_2+', '+a_nb_n', '}', '{', '\\\\sqrt', '{', 'a_1^2', '+', 'a_2^2+', '+a_n^2', '}', '\\\\cdot', '\\\\sqrt', '{', 'b_1^2', '+', 'b_2^2+', '+b_n^2', '}', '}', '\\\\', 'skalarprodukt', 'produkt', 'vektorlängen', 'z.b', 'veranschaulichung', 'folgend', 'vektoren', 'zweidimensional', 'raum', 'miteinander', 'verglichen:3', 'vektoren', 'zweidimensional', 'raumcosine', 'similarity', 'explained', 'using', 'python', '|', 'by', 'misha', 'sv', '|', 'towards', 'data', 'science', '16.03.2022', 'ergeben', 'kosinus-ähnlichkeit', '\\\\', 'ähnlichkeit', 'a', 'b', '=cos', '\\\\theta', '=', '\\\\frac', '{', '1', '\\\\cdot', '2+4', '\\\\cdot', '4', '}', '{', '\\\\sqrt', '{', '1^2', '+4^2', '}', '\\\\cdot', '\\\\sqrt', '{', '2^2', '+', '4^2', '}', '}', '=', '\\\\frac', '{', '18', '}', '{', '\\\\sqrt', '{', '17', '}', '}', '=', '\\\\frac', '{', '18', '}', '{', '4,14', '}', '=0,976\\\\\\\\', 'ähnlichkeit', 'a', 'c', '=', '0,74', '\\\\', 'bedeuten', 'vektoren', 'a', 'b', 'ähnlich', 'a', 'c.', 'je', 'ähnlichkeit', 'co', 'θ', '1', 'gehen', 'ähnlich', 'zwei', 'vektoren', 'index', 'hoch', 'wert', 'comparisons.argmax', 'ermitteln', 'relevant', 'linker', 'indizieren']]\n",
            "['https://eick-at.de/moodle/mod/page/view.php?id=1367', ['datenvisualisierung', 'ki-modell', 'trainieren', 'müssen', 'feststellen', 'datum', 'überhaupt', 'einflussen', 'vorherzusagend', 'größe', 'hängen', 'auswahl', 'modell', 'art', 'datum', 'ab.den', 'zusammenhang', 'merkmal', 'müssen', 'eigentlich', 'immer', 'graphisch', 'untersuchen', 'untersuchung', 'datenreihen', 'tabelle', 'angesichts', 'datumenmenge', 'möglich']]\n",
            "['https://eick-at.de/moodle/course/view.php?id=19#section-3', ['entwicklungsumgebung']]\n",
            "['https://eick-at.de/moodle/mod/page/view.php?id=1368', ['programmiersprache', 'ki-programmierung', 'fallen', 'kurs', 'wahl', 'python', 'stehen', 'ganz', 'reihe', 'programmiersprache', 'ki-entwicklung', 'verfügung', 'bedeutend', 'stark', 'ki-bibliotheken', 'aufführen', 'python', 'einfach', 'lernend', 'verbreitet', 'programmiersprache', 'entwickeln', 'codebasis', 'insbesondere', 'geben', 'leistungsfähig', 'werkzeuge', 'datumenanalyse', 'vielleicht', 'wichtig', 'schlüssel', 'ki-entwicklung', 'groß', 'datumensatz', 'klassifizieren', 'analysieren', 'aufbereiten', 'tensorflow', 'stehen', 'leistungsfähig', 'googeln', 'entwickeln', 'open-source-bibliothek', 'maschinell', 'lernen', 'verfügung', 'helfen', 'schnell', 'relativ', 'einfach', 'modell', 'maschinell', 'lernen', 'erstellen', 'trainieren', 'evaluieren', 'schlussendlich', 'betrieb', 'nehmen.python', 'ki-bibliothek', 'tensorflowjava', 'leistungsfähig', 'alternative', 'dar', 'mehreren', 'plattform', 'einsetzen', 'geben', 'beispiesweise', 'open', 'source-bibliothek', 'deeplearning4j', 'dl4j', 'kommen', 'groß', 'anzahl', 'implementierungsbeispielen', 'ki-entwicklung', 'daneben', 'bieten', 'java', 'einfach', 'debugging', 'benutzer-', 'wartungsfreundlichkeit', 'graphisch', 'benutzeroberfläche', 'usw', 'c++', 'leicht', 'erlernen', 'dafür', 'ressourcensparend', 'schnell', 'ausführung', 'bieten', 'c++', 'besonders', 'gut', 'machine', 'learning', 'leistungsschwach', 'system', 'mikrocontrollern', 'eingebettet', 'system', 'dafür', 'stehen', 'z.b', 'opencv', 'ki-programmbibliothek', 'verfügung', 'programmiersprache', 'u.a', 'python', 'unterstützen', 'neben', 'ml', 'computer', 'vision', 'objekteidentifizierung', 'bilderkennung', 'stark', 'opencv', 'javascript', 'vielleicht', 'beliebt', 'programmiersprache', 'welt', 'internetbrowser', 'ausführen', 'internetseite', 'interaktiv', 'modern', 'web', 'erst', 'möglich', 'javascript', 'verfügen', 'mehrere', 'high-level-tools', '-bibliotheken', 'maschinell', 'lernen', 'gut', 'beispiel', 'googles', 'tensorflow.js', 'webentwicklern', 'viele', 'neu', 'möglichkeit', 'eröffnet', 'neu', 'programmiersprache', 'julia', 'erst', '2009', 'entwickeln', '2018', 'offiziell', 'einführen', 'erfreuen', 'rasch', 'wachsend', 'beliebtheit', 'sprache', 'sollen', 'leistungsfähigkeit', 'einfach', 'programmierung', 'vereinigen', 'parallelverarbeitung', 'auslegen', 'besonders', 'gut', 'wissenschaftlich', 'berechnung', 'datenanalysen', 'geeignet', 'ki-', 'ml-programmpaket', 'julia', 'heißen', 'flux', 'bieten', 'flux', 'option', 'ki-programmbibliotheken', 'gesamt', 'system', 'schnell', 'wachsen', 'sollen', 'entwicklung', 'fall', 'beobachtet', 'lisp', 'fortran', 'bereits', '1958', 'spezifizieren', 'zweitälteste', 'hoch', 'programmiersprache', 'entwicklung', 'eng', 'früh', 'fortschritt', 'ki', 'verbunden', 'lisp', 'ki-projekten', 'weit', 'verbreitet', 'lisp', 'gelten', 'gut', 'prototyping-tool', 'problem', 'deren', 'lösungweg', 'unklar', 'sprache', 'hervorragend', 'umgang', 'liste', 'symbolisch', 'information', 'geeignet', 'letzteres', 'bedeuten', 'lisp', 'besonders', 'gut', 'entwicklung', 'regelbasierter', 'ki-systeme', 'geeignet', 'mathematisch', 'formel', 'wenn-dann-beziehung', 'usw', 'wissen', 'symbolisch', 'repräsentieren', 'grund', 'warum', 'lisp', 'häufig', 'verwendet', 'programmiersprache', 'ki', 'liegen', 'teil', 'daran', 'gut', 'entwickeln', 'modern', 'machine', 'learning-bibliotheken', 'ml', '=', 'nicht-', 'subsymbolisch', 'ansatz', 'geben', 'syntax', 'zudem', 'stark', 'programmiersprache', 'unterscheiden', 'quelle', 'https', '//bootcamp.berkeley.edu/blog/ai-programming-languages/', '07.02.2022']]\n",
            "['https://eick-at.de/moodle/mod/page/view.php?id=1369', ['anaconda', 'anaconda', 'distribution', 'quelloffene', 'python-entwicklungswerkzeuge', 'ki-entwicklung', 'sollen', 'software-paketmanagement', '-verteilung', 'vereinfachen.die', 'individual', 'edition', 'nicht-kommerziell', 'gebrauch', 'kostenlos', 'enthalten', 'u.a', 'folgend', 'programmpaket']]\n",
            "['https://eick-at.de/moodle/mod/page/view.php?id=1370', ['jupyter', 'notebook', 'öffnen', 'notebooks', 'cli', 'windowsto', 'doquelle', 'https', '//realpython.com/jupyter-notebook-introduction/', '07.02.2022', 'beim', 'jupyter-notebook', 'client-server-anwendung', 'handeln', 'liegen', 'natürlich', 'nahe', 'jupyter-notebooks', 'internet', 'verfügbar', 'geben', 'vielzahl', 'jupyter-notebook-anbietern', 'wichtig']]\n",
            "['https://eick-at.de/moodle/course/view.php?id=19#section-4', ['quelle']]\n",
            "['https://eick-at.de/moodle/course/view.php?id=19#section-5', ['ki-domainen']]\n",
            "['https://eick-at.de/moodle/mod/page/view.php?id=1373', ['computer', 'vision', 'klassifikationproblem', 'hund', 'schnee', 'huskys']]\n",
            "['https://eick-at.de/moodle/mod/page/view.php?id=1374', ['natural', 'language', 'processing', 'nlp', 'abkürzung', 'natural', 'language', 'processing', 'beschreiben', 'technik', 'methode', 'spracherkennung', 'verarbeitung', 'natürlich', 'sprache', 'verarbeitung', 'finden', 'hierbei', 'maschinell', 'statt', 'ziel', 'direkt', 'natürlich', 'kommunikation', 'mensch', 'maschine', 'ermöglichen', '–', 'schriftlich', 'mündlich', 'form']]\n",
            "['https://eick-at.de/moodle/course/view.php?id=19#section-6', ['machine', 'learning']]\n",
            "['https://eick-at.de/moodle/mod/page/view.php?id=1375', ['teachable', 'machine', 'to', 'do']]\n"
          ]
        }
      ],
      "source": [
        "data = []                                      \n",
        "for header in headers:                          # iterate sections\n",
        "  # print(header)\n",
        "  link = header.find('a').attrs['href']         # extract link of section\n",
        "  txt = header.get_text()                       # extract text of section\n",
        "  processed = processText(txt)                  # nlp of section text\n",
        "  entry = [link, processed]                     # list entry with link and processed text\n",
        "  print(entry)\n",
        "  data.append(entry)                            # append entry in list data (nested lists)\n",
        "\n",
        "  listTags = header.find_next_sibling('ul')     # pages, tasks, links ..\n",
        "  for li in listTags:\n",
        "    #print(li)\n",
        "    link = li.find('a')\n",
        "    txt2 = link.get_text().replace(\"Page\",\"\").replace(\"URL\",\"\").replace(\"Assignment\",\"\")\n",
        "    if not(\"URL\" in link.get_text()):\n",
        "      link = link.attrs['href'] \n",
        "      \n",
        "      r = requests.get(link)                    # Seiten aufrufen\n",
        "      soup2 = bs4.BeautifulSoup(r.text,'html5lib')\n",
        "      header2 = soup2.find('h2')                # Überschrift\n",
        "      #print(header2)\n",
        "      paragraphs = header2.find_all_next('p')   # Absätze\n",
        "      txt = \"\"\n",
        "      for paragraph in paragraphs:\n",
        "        txt += paragraph.get_text()\n",
        "      txt2 += txt\n",
        "      #print(link,txt2)\n",
        "      processed =processText(txt2)\n",
        "      entry = [link, processed]\n",
        "      print(entry)\n",
        "      data.append(entry)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AnEogOIxVcnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_hist(data):\n",
        "    entry_lengths = [len(entry[1]) for entry in data]\n",
        "    fig = plt.figure(figsize=(6, 6)) \n",
        "    plt.xlabel('Eintraglänge')\n",
        "    plt.ylabel('Anzahl der Einträge')\n",
        "    plt.hist(entry_lengths, bins=20)\n",
        "    plt.show()\n",
        "    return entry_lengths\n",
        "entry_lengths = plot_hist(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "bS3JjzO67I5R",
        "outputId": "c0e0c0f4-3936-4aff-f2a5-bd8a4163e8ec"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of words"
      ],
      "metadata": {
        "id": "q87hHeZyDRMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate frequency of words\n",
        "def map_book(hash_map, tokens):\n",
        "    if tokens is not None:\n",
        "        for word in tokens:\n",
        "            # Word Exist?\n",
        "            if word in hash_map:\n",
        "                hash_map[word] = hash_map[word] + 1\n",
        "            else:\n",
        "                hash_map[word] = 1\n",
        "\n",
        "        return hash_map\n",
        "    else:\n",
        "        return None\n",
        "        \n",
        "def make_hash_map(data):  \n",
        "    hash_map = {}\n",
        "    for entry in data:\n",
        "        hash_map = map_book(hash_map, entry[1])\n",
        "    return hash_map\n",
        "\n",
        "\n",
        "# define a function frequent_vocab with the following input: word_freq and max_features\n",
        "def frequent_vocab(word_freq, max_features): \n",
        "    counter = 0  #initialize counter with the value zero\n",
        "    vocab = []   # create an empty list called vocab\n",
        "    # list words in the dictionary in descending order of frequency\n",
        "    for key, value in sorted(word_freq.items(), key=lambda item: (item[1], item[0]), reverse=True): \n",
        "       #loop function to get the top (max_features) number of words\n",
        "        if counter<max_features: \n",
        "            vocab.append(key)\n",
        "            counter+=1\n",
        "        else: break\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "UQlsfmIe7M7x"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hash_map = make_hash_map(data) #create hash map (words and frequency) from tokenized dataset\n",
        "\n",
        "vocab=frequent_vocab(hash_map, 1000)  # adjust second Parameter\n",
        "print(hash_map)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EorQNc7vQZXY",
        "outputId": "11e60a90-4b38-41e0-d07b-90435b13d290"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'allgemein': 1, 'information': 2, 'einführung': 1, 'künstlich': 15, 'intelligenz': 9, 'ki': 8, 'engl': 5, 'artificial': 2, 'intelligence': 2, 'ai': 1, 'bezeichnen': 2, 'teilgebiet': 2, 'informatik': 1, 'automatisierung': 1, 'intelligent': 2, 'verhalten': 1, 'maschinell': 13, 'lernen': 11, 'befassen': 1, 'begriff': 7, 'verwendet': 2, 'obwohl': 1, 'zugrunde': 1, 'liegend': 1, 'definition': 1, 'durchaus': 3, 'umstritten': 1, 'traditionell': 2, 'art': 3, 'realisert': 1, 'lange': 1, 'reihe': 2, 'wenn-dann-anweisung': 1, 'einfach': 5, 'kontrollstrukturen': 1, 'bzw': 5, 'regel': 2, 'aneinander': 1, 'reihen': 1, 'expertensystem': 1, 'lassen': 6, 'z.b': 11, 'krankheit': 1, 'anhand': 4, 'symptom': 1, 'diagnostizieren': 1, 'gewissen': 2, 'grenze': 1, 'klassisch': 1, 'programmierung': 2, 'erreichen': 2, 'allerdings': 4, 'algorithmen': 2, 'je': 3, 'anforderung': 2, 'kompliziert': 1, 'programmierungbeim': 1, 'dagegen': 3, 'maschine': 2, 'automatisch': 2, 'sammlung': 1, 'bekannte': 1, 'beispiel': 2, 'trainieren': 7, 'heute': 1, 'gängig': 1, 'methode': 3, 'lernendabei': 1, 'deep': 5, 'learning': 8, 'besonderer': 1, 'form': 2, 'neuronal': 5, 'netz': 6, 'knn': 2, 'programmatisch': 1, 'nachbildung': 1, 'vernetzen': 2, 'gehirnzell': 2, 'sog': 1, 'neuronen': 6, 'datum': 8, 'erwartet': 1, 'ergebnis': 4, 'abgleichen': 2, 'dabei': 9, 'nerven-': 1, 'verbindung': 2, 'unterschiedlich': 2, 'gewichtet': 5, 'verstärken': 2, 'bias': 1, 'tief': 1, 'viele': 4, 'hidden': 1, 'lay': 1, 'aufweisen': 2, 'gewiß': 1, 'komplexität': 1, 'wiederum': 1, 'teilbereich': 1, 'darstellen': 2, 'zusammenhang': 3, 'machine': 6, 'aufgabe': 3, 'anwendungsgebiet': 2, 'kennen': 1, 'lösung': 1, 'ki-anwendungsgebiete': 1, 'quelle': 4, 'the': 1, 'difference': 1, 'between': 1, 'and': 1, 'intel': 1, 'communities': 1, '31.12.2021': 1, 'heizung': 2, 'anwesenheitsdatum': 1, 'bewohner': 3, 'verlassen': 1, 'letzter': 1, 'abschalten': 1, 'vorhersagen': 2, 'wann': 1, 'eintreffen': 1, 'erster': 1, 'rechnen': 1, 'heizen': 1, 'entsprechend': 3, 'wohnung': 1, 'spielen': 2, 'probieren': 1, 'folgend': 6, 'ki-spiele': 1, 'online': 1, 'semantrisquick': 1, 'draw': 1, 'rock': 1, 'pap': 1, 'scissorsdiskutiere': 1, 'notieren': 1, 'bestmöglich': 1, 'strategie': 1, 'partnerarbeit': 1, 'biologisch': 1, 'neuron': 10, 'nervenzell': 2, 'nervensystem': 1, 'bestehen': 2, 'zusammen': 1, 'gliazell': 1, 'menschlich': 2, 'gehirn': 2, 'enthalten': 7, 'ca': 1, '90': 1, 'milliarde': 1, 'empfangen': 1, 'signal': 3, 'mehreren': 2, 'erregung': 3, 'aufsummieren': 1, 'bestimmt': 2, 'schwellenpotential': 1, 'überschreiten': 1, '-prinzip': 1, 'seinerseits': 1, 'feuer': 1, 'd.h.': 1, 'erregen': 1, 'zelle': 2, 'denen': 3, 'nervenfasern': 1, 'fest': 1, 'verbunden': 3, 'natürlich': 8, 'synapse': 1, 'https': 8, '//nl.dreamstime.com/transmissie-van-het-zenuwsignaal-tussen-twee-neuronen-met-axon-en-synaps-close-up-een-chemische-neurale-mededeling-vectordiagram-image153760677': 1, 'bearbeiten': 1, '20.02.2022': 1, 'synapsen': 3, 'wichtig': 6, 'rolle': 1, 'übergangsstelle': 1, 'übertragen': 4, 'überwiegend': 1, 'chemisch': 2, 'neurotransmitter': 1, 'synaptisch': 1, 'spalt': 1, 'anpassungsfähig': 1, 'menge': 1, 'botenstoff': 1, 'effekt': 1, 'effektivität': 1, 'schnittstelle': 1, 'nutzung': 1, 'abschwächen': 1, 'vorhanden': 3, 'fehlend': 1, 'training': 1, 'befinden': 1, 'schätzen': 1, '1': 6, 'billiarde': 1, 'nervenverbindung': 1, 'informationstechnisch': 1, 'stark': 8, 'vereinfachen': 1, 'nachbilden': 1, 'feueren': 1, 'wesentlich': 2, 'einzeln': 7, 'berechnen': 4, 'gegensatz': 1, 'laufen': 1, 'aufsummierung': 1, 'eingehend': 2, 'gleichzeitig': 2, 'parallel': 1, 'ab': 1, 'gewichtung': 4, '//medium.com/': 1, '@': 1, 'najeebnik21/activation-function-in-deep-learning-587e83d5a681': 1, '17.02.2022': 1, 'genauso': 1, 'schrittweise': 1, 'bedeuten': 4, 'lernprozess': 1, 'zeitlich': 1, 'nacheinander': 1, 'neu': 4, 'müssen': 15, 'echt': 1, 'parallelverarbeitung': 3, 'rechenkerne': 1, 'beschleunigen': 1, 'sowohl': 1, 'verarbeitungs-': 1, 'trainingsprozess': 1, 'graphic': 1, 'processing': 6, 'unit': 4, 'gpu': 3, 'vorteil': 1, 'verfügen': 2, 'hundert': 1, 'tausend': 1, 'cor': 1, 'wegen': 1, 'graphisch': 4, 'berechnung': 3, 'durchsatz': 1, 'hoch': 3, 'verarbeitungsgeschwindigkeit': 1, 'befehl': 1, 'central': 1, 'cpus': 1, 'optimieren': 2, 'jetson': 2, 'nano': 2, 'nvidia': 1, '128-kern': 2, 'resourcensparender': 1, 'einplatinen-rechner': 1, 'ki-projekte': 1, 'cpu-leistung': 1, 'etwa': 1, 'rasperry': 1, 'pi': 1, 'vergleichbar': 1, '//developer.nvidia.com/embedded/dlc/jetson-nano-system-module-datasheettensor': 1, 'tpus': 3, 'anwendungsspezifisch': 1, 'chip': 1, 'asics': 1, 'speziell': 2, 'entwerfen': 2, 'compute': 1, 'unterstützen': 2, 'notwendig': 1, 'matrizenrechnung': 1, 'fließkomma-berechnung': 1, 'realisieren': 1, 'ganz': 2, 'besonders': 4, 'leistungsfähig': 4, 'googeln': 3, 'entwickeln': 5, 'softwaresammlung': 1, 'tensorflow': 2, 'unternehmen': 1, 'basis': 3, 'services': 1, 'einsetzen': 2, 'datenexploration': 1, 'data-mining': 6, 'eigentlich': 3, 'missverständlich': 1, 'gehen': 5, 'förderung': 1, 'to': 4, 'mine': 1, '=': 17, 'graben': 1, 'abbau': 1, 'fördern': 1, 'untersuchung': 2, 'bereits': 4, 'groß': 5, 'datenmengen': 1, 'bestimmen': 3, 'trend': 1, 'regelmäßigkeit': 1, 'gesetzmäßigkeit': 1, 'abweichung': 1, 'usw': 5, 'hierfür': 1, 'oft': 4, 'experimentell': 1, 'statistisch': 1, 'aufbereitung': 1, 'eingesetzt.gelegentlich': 1, 'verstehen': 1, 'darunter': 1, 'zusätzlich': 2, 'erfassung': 1, 'mittels': 1, 'web': 3, 'scraping': 2, 'speicherung': 1, 'aufarbeitung': 1, 'wissenschaftlich': 2, 'sinn': 1, 'vorbereitend': 1, 'prozessschritte': 1, 'sogenannter': 3, 'knowledge': 1, 'discovery': 1, 'databases': 1, 'prozeß': 1, 'schritt': 2, 'data': 3, 'mining': 1, 'kdd-prozessdata-mining': 1, 'vielleicht': 3, 'deutsch': 2, 'ehesten': 1, 'datenmustererkennung': 1, '-entdeckung': 1, 'bezeichnen.data-mining': 1, 'entwicklung': 4, 'darum': 1, 'herausfinden': 1, 'feature': 4, 'datenmerkmalen': 1, 'ende': 1, 'gültig': 1, 'erhalten': 1, 'like-button': 3, 'facebook': 4, 'hören': 1, 'vortrag': 2, 'david': 2, 'kriesels': 1, 'spiegel-mining': 1, 'analysieren': 3, 'diskutieren': 1, 'beschreiben': 3, 'möglichkeit': 2, 'gefahr': 1, 'hinblick': 1, 'lösungsvorschlag': 1, 'kriesel': 1, 'umreißen': 1, 'scenario': 1, 'likes': 3, 'benutzer': 1, 'einstellung': 2, 'vorliebe': 1, 'schließen': 1, 'gespeichert': 1, 'personell': 1, 'alter': 1, 'geschlecht': 1, 'allein': 1, 'sicher': 1, 'rückschluß': 1, 'darauf': 1, 'vornehmen': 2, 'aufruf': 2, 'jeweilig': 2, 'facebook-seit': 2, 'unbedingt': 1, 'bild': 1, 'komplett': 1, 'aussagen': 1, 'inhalt': 3, 'fällen': 1, 'verweildauer': 1, 'nächster': 2, 'messen': 1, 'absolut': 1, 'anzahl': 3, 'facebook-seitenaufrufe': 1, 'speichern': 3, 'umstand': 1, 'weitere': 1, 'nützlich': 1, '.der': 1, 'like': 1, 'binäres': 1, 'unklar': 2, 'bleiben': 2, 'negativ': 1, 'wert': 5, 'egal': 1, 'interessieren': 1, 'positiv': 1, 'nutzer': 1, 'aktiv': 1, 'zustimmung': 1, 'erklären': 1, 'zahl': 1, 'weiteres': 1, 'auswerten': 2, 'demnach': 1, 'mindestanzahl': 1, 'wahrscheinlich': 1, 'gut': 8, 'prognose': 1, 'abgeben': 1, 'heißen': 3, 'politisch': 1, 'bevorzugen': 1, 'ansprache': 1, 'duzen': 1, 'siezen': 1, 'vorhersage': 1, 'personalisiert': 1, 'wahlwerbung': 1, 'möglich': 4, 'demokratie': 1, 'weit': 3, 'manipulation': 1, 'kaufverhalten': 1, 'hinaus': 1, 'kritisch': 1, 'beobachtet': 2, 'nlp-basiert': 1, 'suchmaschine': 3, 'normalerweise': 1, 'basieren': 1, 'suchen': 6, 'innerhalb': 3, 'webauftritts': 2, 'sql-datenbankabfragen': 1, 'suchworte': 2, 'seite': 15, 'zuordnen': 2, 'können.beispiel': 1, 'codeauszug': 1, 'wordpress-installation': 1, 'wpdb': 1, ':esc_like': 1, '|': 4, 'method': 1, 'wordpress': 1, 'developer': 1, 'resources': 1, '10.03.2022': 1, 'zugang': 1, 'datumenbank': 1, 'internet': 3, 'service': 1, 'provider': 1, 'webseiten': 3, 'zwangsläufig': 1, 'lesen': 2, 'internet-robots': 1, 'kurz': 1, 'bot': 2, 'programm': 1, 'selbstständig': 1, 'internetlinks': 1, 'folgen': 1, 'suchmaschinen-anbieter': 2, 'auslesen': 2, 'crawler': 2, 'quasi': 1, 'kriechen': 1, 'crawl': 1, 'automatisiert': 1, 'kratzen': 1, 'scrape': 1, 'bezeichnet.die': 1, 'webseiteninhalte': 1, 'aufbereiten': 2, 'abspeichern': 1, 'bereit': 1, 'text': 3, 'hilfe': 1, 'natural': 4, 'language': 4, 'vergleich': 2, 'erfolgen': 2, 'danach': 1, 'verfahren': 2, 'beispielhaft': 1, 'anwenden': 3, 'moodle-kurs': 1, '//eick-at.de': 1, 'öffentlich': 1, 'spiegeln': 1, 'programmieren': 2, '//github.com/ateachment/moodlesuche/blob/f88365d6eaa65f0a5eb718bc25ef7530f0181629/moodlesuche.ipynbzunächst': 1, 'strartseit': 1, 'kurs': 6, 'aufgerufen': 1, 'r.text': 1, 'unbearbeitet': 1, 'python-bibliothek': 3, 'beautiful': 1, 'soup': 1, 'html': 1, 'parser': 2, 'html-code': 2, 'startseite': 3, 'html5lib': 1, 'weisen': 1, 'beautifulsoup': 1, 'html-information': 1, 'inhaltsverzeichnis': 1, 'inspektion': 1, 'ergeben': 4, 'abschnitt': 2, 'h3-element': 1, 'überschrift': 1, 'heading': 1, '3': 1, 'ordnung': 1, 'gekennzeichnet': 1, '<': 3, 'h3': 1, '>': 3, '-tags': 2, 'html-': 1, 'suppe': 1, 'liste': 7, 'headers': 1, 'kopieren': 1, 'listenelement': 1, 'head': 2, 'schleife': 2, 'durchlaufen': 4, 'links': 4, 'sowie': 1, 'link-texte': 2, 'extrahiert': 2, 'nlp': 3, 'verarbeiten': 1, 'beides': 1, 'abgelegt.jetzt': 1, 'html-inspektion': 1, 'zeigen': 1, 'eintrag': 1, 'listenpunkt': 1, 'ungeordneten': 1, 'aufführen': 2, 'zunächst': 2, 'übergeordnet': 1, 'ul': 1, '-tag': 1, 'unordered': 1, 'listen': 2, 'li': 1, 'item': 1, 'extern': 1, 'url': 1, 'verweisen': 1, 'prinzipiell': 1, 'gleich': 1, 'parameter': 3, 'txt': 1, 'html-tags': 1, 'mehr': 1, 'lauten': 1, 'z.b.:1': 1, 'wort-tokenisierung': 1, 'nltk': 1, 'toolkit': 1, 'trennen': 1, 'leerzeich': 1, 'satzzeichen': 3, 'wort': 24, 'geschrieben:2': 1, 'hanover-lemmatizer': 1, 'sprache': 4, 'ersetzen': 1, 'reduzieren': 1, 'einzahl': 1, 'grund-': 1, 'stammform:3': 1, '+': 4, '4': 2, 'abschließend': 1, 'stop-wörter': 1, 'entfernt': 1, 'relevanz': 5, 'besitz': 1, 'außerdem': 3, 'durchgehend': 1, 'kleinschreibung': 1, 'darin': 1, 'häufigkeit': 2, 'gesamt': 4, 'wortschatz': 3, 'sehen': 2, 'folgendermaßen': 2, 'hashmap': 1, 'zuordnung': 1, 'schlüssel': 2, 'key': 1, \"'künstlich\": 2, \"'\": 3, \"'14\": 1, 'kommen': 4, 'genau': 1, '14': 1, 'mal': 2, 'vor.jetzt': 1, 'vorkommen': 13, 'absteigend': 1, 'sortieren': 1, 'zweiter': 1, 'begrenzen': 2, 'lang': 1, 'datumenmenge': 2, 'rechenaufwand': 1, 'davon': 1, 'ausgehen': 1, 'selten': 2, 'vorkommend': 2, 'weniger': 2, 'relevant': 4, 'eher': 1, 'abgeschnitten': 1, 'dürfen': 1, 'häufig': 3, 'vorne': 1, 'stehen': 5, 'verarbeitung': 4, 'ausschließen': 1, 'sorgfältig': 1, 'abhängigkeit': 1, 'rechenleistung': 1, 'verarbeitungsdauer': 1, 'anpassen': 2, 'werden.das': 1, 'mathematisch': 3, 'direkt': 2, 'erst': 4, 'zahlen': 1, 'umwandeln': 1, 'dafür': 5, 'bag': 7, 'of': 9, 'words': 8, 'erstellen': 3, 'liefern': 1, 'folgendes': 1, 'beiden': 2, 'überhaupt': 4, '10': 1, 'stelle': 1, 'array': 2, '1000': 1, 'element': 1, 'stellen': 1, 'vektor': 1, '1000-dimensionalen': 1, 'raum': 2, 'dar': 2, 'vektoren': 6, 'miteinander': 2, 'vergleichen': 2, 'ähnlichkeit': 4, 'können.dazu': 1, 'gescrapte': 1, '2-dimensionales': 1, 'bag_o': 1, 'erzeugen': 1, 'zeile': 1, 'bevor': 1, 'such-bag': 1, 'berücksichtigen': 2, 'abbilden': 1, 'diejenigen': 1, 'suchfunktion': 1, 'aussagekräftiger.die': 1, 'wenigen': 2, 'vorkommt.tf-idf': 1, 'term': 5, 'frequency': 6, 'inverse': 4, 'document': 3, 'records': 1, 'umgangssprachlich': 1, 'bedeutung': 3, '.die': 1, 'termhäufigkeit': 1, 'tf': 5, 'dokument': 10, 'öfter': 1, '\\\\': 16, '\\\\frac': 9, '{': 28, 'anzahl\\\\': 2, 'eines\\\\': 4, 'wortes\\\\': 4, 'im\\\\': 1, '}': 28, 'gesamtwortzahl\\\\': 1, 'des\\\\': 1, 'dokumentenhäufigkeit': 2, 'df': 6, 'besagen': 1, 'der\\\\': 1, 'dokumente\\\\': 2, 'in\\\\': 1, 'denen\\\\': 1, 'ein\\\\': 1, 'bestimmtes\\\\': 1, 'wort\\\\': 1, 'umkehren': 2, 'proportional': 3, 'relevanz\\\\': 3, '\\\\sim': 2, 'wieviele': 1, 'geben': 5, 'n': 4, 'gesamtzahl\\\\': 1, 'aller\\\\': 1, 'schlussfolgerung': 1, 'idf': 3, 'tatsächlich': 1, '0': 3, 'völlig': 1, 'unbrauchbar': 1, 'logarithmus': 1, '2': 2, 'bruch': 1, 'gelten': 2, 'log': 1, 'logarithmusfunktion': 1, '//en.wikipedia.org/wiki/binary_logarithm': 1, '12.03.2022': 1, 'jedoch': 1, 'mulitpliziert': 1, 'tfidf': 2, '\\\\cdot': 7, 'code': 1, 'manuell': 1, 'sklearn': 1, 'bieten': 4, 'fertig': 1, 'algorithmus': 1, 'gewichtet.diese': 1, 'webcrawling': 1, 'ermitteln': 2, 'zeitgesteuerter': 1, 'task': 1, 'tag': 1, 'erledigen': 1, 'webcrawler': 1, 'vorzugsweise': 1, 'netzwerkbelastung': 1, 'gering': 1, 'kompakt': 1, 'binären': 1, 'format': 1, 'mensch': 2, 'lesbar': 1, 'müssen.das': 1, 'datei': 1, 'such-vektor': 1, 'seiten-vektoren': 1, 'jeweils': 1, 'kosinus-ähnlichkeit': 2, 'co': 2, '\\\\theta': 2, '\\\\sigma': 3, 'a_i': 1, 'b_i': 1, '\\\\sqrt': 7, 'a_i^2': 1, 'b_i^2': 1, 'a_1b_1+a_2b_2+': 1, '+a_nb_n': 1, 'a_1^2': 1, 'a_2^2+': 1, '+a_n^2': 1, 'b_1^2': 1, 'b_2^2+': 1, '+b_n^2': 1, 'skalarprodukt': 1, 'produkt': 1, 'vektorlängen': 1, 'veranschaulichung': 1, 'zweidimensional': 2, 'verglichen:3': 1, 'raumcosine': 1, 'similarity': 1, 'explained': 1, 'using': 1, 'python': 4, 'by': 1, 'misha': 1, 'sv': 1, 'towards': 1, 'science': 1, '16.03.2022': 1, 'a': 4, 'b': 2, '=cos': 1, '2+4': 1, '1^2': 1, '+4^2': 1, '2^2': 1, '4^2': 1, '18': 2, '17': 1, '4,14': 1, '=0,976\\\\\\\\': 1, 'c': 1, '0,74': 1, 'ähnlich': 2, 'c.': 1, 'θ': 1, 'zwei': 1, 'index': 1, 'comparisons.argmax': 1, 'linker': 1, 'indizieren': 1, 'datenvisualisierung': 1, 'ki-modell': 1, 'feststellen': 1, 'einflussen': 1, 'vorherzusagend': 1, 'größe': 1, 'hängen': 1, 'auswahl': 1, 'modell': 2, 'ab.den': 1, 'merkmal': 1, 'immer': 1, 'untersuchen': 1, 'datenreihen': 1, 'tabelle': 1, 'angesichts': 1, 'entwicklungsumgebung': 1, 'programmiersprache': 9, 'ki-programmierung': 1, 'fallen': 1, 'wahl': 1, 'ki-entwicklung': 4, 'verfügung': 3, 'bedeutend': 1, 'ki-bibliotheken': 1, 'lernend': 1, 'verbreitet': 2, 'codebasis': 1, 'insbesondere': 1, 'werkzeuge': 1, 'datumenanalyse': 1, 'datumensatz': 1, 'klassifizieren': 1, 'open-source-bibliothek': 1, 'helfen': 1, 'schnell': 3, 'relativ': 1, 'evaluieren': 1, 'schlussendlich': 1, 'betrieb': 1, 'nehmen.python': 1, 'ki-bibliothek': 1, 'tensorflowjava': 1, 'alternative': 1, 'plattform': 1, 'beispiesweise': 1, 'open': 1, 'source-bibliothek': 1, 'deeplearning4j': 1, 'dl4j': 1, 'implementierungsbeispielen': 1, 'daneben': 1, 'java': 1, 'debugging': 1, 'benutzer-': 1, 'wartungsfreundlichkeit': 1, 'benutzeroberfläche': 1, 'c++': 2, 'leicht': 1, 'erlernen': 1, 'ressourcensparend': 1, 'ausführung': 1, 'leistungsschwach': 1, 'system': 3, 'mikrocontrollern': 1, 'eingebettet': 1, 'opencv': 2, 'ki-programmbibliothek': 1, 'u.a': 2, 'neben': 1, 'ml': 2, 'computer': 2, 'vision': 2, 'objekteidentifizierung': 1, 'bilderkennung': 1, 'javascript': 2, 'beliebt': 1, 'welt': 1, 'internetbrowser': 1, 'ausführen': 1, 'internetseite': 1, 'interaktiv': 1, 'modern': 2, 'mehrere': 1, 'high-level-tools': 1, '-bibliotheken': 1, 'googles': 1, 'tensorflow.js': 1, 'webentwicklern': 1, 'eröffnet': 1, 'julia': 2, '2009': 1, '2018': 1, 'offiziell': 1, 'einführen': 1, 'erfreuen': 1, 'rasch': 1, 'wachsend': 1, 'beliebtheit': 1, 'sollen': 3, 'leistungsfähigkeit': 1, 'vereinigen': 1, 'auslegen': 1, 'datenanalysen': 1, 'geeignet': 3, 'ki-': 1, 'ml-programmpaket': 1, 'flux': 2, 'option': 1, 'ki-programmbibliotheken': 1, 'wachsen': 1, 'fall': 1, 'lisp': 5, 'fortran': 1, '1958': 1, 'spezifizieren': 1, 'zweitälteste': 1, 'eng': 1, 'früh': 1, 'fortschritt': 1, 'ki-projekten': 1, 'prototyping-tool': 1, 'problem': 1, 'deren': 1, 'lösungweg': 1, 'hervorragend': 1, 'umgang': 1, 'symbolisch': 2, 'letzteres': 1, 'regelbasierter': 1, 'ki-systeme': 1, 'formel': 1, 'wenn-dann-beziehung': 1, 'wissen': 1, 'repräsentieren': 1, 'grund': 1, 'warum': 1, 'liegen': 2, 'teil': 1, 'daran': 1, 'learning-bibliotheken': 1, 'nicht-': 1, 'subsymbolisch': 1, 'ansatz': 1, 'syntax': 1, 'zudem': 1, 'unterscheiden': 1, '//bootcamp.berkeley.edu/blog/ai-programming-languages/': 1, '07.02.2022': 2, 'anaconda': 2, 'distribution': 1, 'quelloffene': 1, 'python-entwicklungswerkzeuge': 1, 'software-paketmanagement': 1, '-verteilung': 1, 'vereinfachen.die': 1, 'individual': 1, 'edition': 1, 'nicht-kommerziell': 1, 'gebrauch': 1, 'kostenlos': 1, 'programmpaket': 1, 'jupyter': 1, 'notebook': 1, 'öffnen': 1, 'notebooks': 1, 'cli': 1, 'windowsto': 1, 'doquelle': 1, '//realpython.com/jupyter-notebook-introduction/': 1, 'beim': 1, 'jupyter-notebook': 1, 'client-server-anwendung': 1, 'handeln': 1, 'nahe': 1, 'jupyter-notebooks': 1, 'verfügbar': 1, 'vielzahl': 1, 'jupyter-notebook-anbietern': 1, 'ki-domainen': 1, 'klassifikationproblem': 1, 'hund': 1, 'schnee': 1, 'huskys': 1, 'abkürzung': 1, 'technik': 1, 'spracherkennung': 1, 'finden': 1, 'hierbei': 1, 'statt': 1, 'ziel': 1, 'kommunikation': 1, 'ermöglichen': 1, '–': 1, 'schriftlich': 1, 'mündlich': 1, 'teachable': 1, 'do': 1}\n",
            "['}', '{', 'wort', '=', '\\\\', 'seite', 'müssen', 'künstlich', 'vorkommen', 'maschinell', 'z.b', 'lernen', 'neuron', 'dokument', 'programmiersprache', 'of', 'intelligenz', 'dabei', '\\\\frac', 'words', 'stark', 'natürlich', 'learning', 'ki', 'https', 'gut', 'datum', 'trainieren', 'liste', 'enthalten', 'einzeln', 'begriff', 'bag', '\\\\sqrt', '\\\\cdot', 'wichtig', 'vektoren', 'suchen', 'processing', 'neuronen', 'netz', 'machine', 'lassen', 'kurs', 'frequency', 'folgend', 'df', 'data-mining', '1', 'wert', 'usw', 'tf', 'term', 'stehen', 'relevanz', 'neuronal', 'lisp', 'groß', 'gewichtet', 'gehen', 'geben', 'entwickeln', 'engl', 'einfach', 'deep', 'dafür', 'bzw', 'übertragen', 'überhaupt', 'ähnlichkeit', '|', 'wortes\\\\', 'viele', 'verarbeitung', 'unit', 'to', 'sprache', 'relevant', 'quelle', 'python', 'oft', 'neu', 'natural', 'n', 'möglich', 'links', 'leistungsfähig', 'language', 'kommen', 'ki-entwicklung', 'inverse', 'graphisch', 'gewichtung', 'gesamt', 'feature', 'facebook', 'erst', 'ergebnis', 'ergeben', 'entwicklung', 'eines\\\\', 'durchlaufen', 'bieten', 'besonders', 'bereits', 'berechnen', 'bedeuten', 'anhand', 'allerdings', 'a', '+', 'zusammenhang', 'wortschatz', 'weit', 'webseiten', 'web', 'vorhanden', 'vielleicht', 'verfügung', 'verbunden', 'tpus', 'text', 'system', 'synapsen', 'suchmaschine', 'startseite', 'speichern', 'sollen', 'sogenannter', 'signal', 'schnell', 'satzzeichen', 'relevanz\\\\', 'python-bibliothek', 'proportional', 'parameter', 'parallelverarbeitung', 'nlp', 'methode', 'mathematisch', 'likes', 'like-button', 'je', 'internet', 'innerhalb', 'inhalt', 'idf', 'häufig', 'hoch', 'heißen', 'gpu', 'googeln', 'geeignet', 'erstellen', 'erregung', 'entsprechend', 'eigentlich', 'durchaus', 'document', 'denen', 'data', 'dagegen', 'bewohner', 'bestimmen', 'beschreiben', 'berechnung', 'bedeutung', 'basis', 'außerdem', 'aufgabe', 'art', 'anzahl', 'anwenden', 'analysieren', '\\\\sigma', '>', '<', '0', \"'\", 'ähnlich', 'zweidimensional', 'zusätzlich', 'zuordnen', 'zunächst', 'zelle', 'wissenschaftlich', 'wesentlich', 'weniger', 'wenigen', 'webauftritts', 'vortrag', 'vornehmen', 'vorkommend', 'vorhersagen', 'vision', 'verwendet', 'verstärken', 'vernetzen', 'vergleichen', 'vergleich', 'verfügen', 'verfahren', 'verbreitet', 'verbindung', 'untersuchung', 'unterstützen', 'unterschiedlich', 'unklar', 'umkehren', 'u.a', 'traditionell', 'tfidf', 'tensorflow', 'teilgebiet', 'symbolisch', 'suchworte', 'suchmaschinen-anbieter', 'spielen', 'speziell', 'selten', 'sehen', 'scraping', 'schritt', 'schlüssel', 'schleife', 'reihe', 'regel', 'raum', 'programmierung', 'programmieren', 'parser', 'optimieren', 'opencv', 'nächster', 'nervenzell', 'nano', 'möglichkeit', 'modern', 'modell', 'ml', 'miteinander', 'menschlich', 'mensch', 'mehreren', 'maschine', 'mal', 'listen', 'link-texte', 'liegen', 'lesen', 'kosinus-ähnlichkeit', 'knn', 'julia', 'jeweilig', 'jetson', 'javascript', 'intelligent', 'intelligence', 'information', 'häufigkeit', 'html-code', 'heizung', 'head', 'gleichzeitig', 'gewissen', 'gelten', 'gehirnzell', 'gehirn', 'ganz', 'form', 'folgendermaßen', 'flux', 'facebook-seit', 'extrahiert', 'erreichen', 'ermitteln', 'erfolgen', 'entwerfen', 'einstellung', 'einsetzen', 'eingehend', 'dokumentenhäufigkeit', 'dokumente\\\\', 'direkt', 'deutsch', 'david', 'datumenmenge', 'darstellen', 'dar', 'crawler', 'computer', 'co', 'chemisch', 'c++', 'bot', 'bleiben', 'bezeichnen', 'bestimmt', 'bestehen', 'berücksichtigen', 'beobachtet', 'beispiel', 'beiden', 'begrenzen', 'b', 'automatisch', 'auswerten', 'auslesen', 'aufweisen', 'aufruf', 'aufführen', 'aufbereiten', 'artificial', 'array', 'anzahl\\\\', 'anwendungsgebiet', 'anpassen', 'anforderung', 'anaconda', 'algorithmen', 'abschnitt', 'abgleichen', '\\\\theta', '\\\\sim', '4', '2', '18', '128-kern', '07.02.2022', '-tags', \"'künstlich\", '–', 'θ', 'überwiegend', 'überschrift', 'überschreiten', 'übergeordnet', 'übergangsstelle', 'öfter', 'öffnen', 'öffentlich', 'zweitälteste', 'zweiter', 'zwei', 'zwangsläufig', 'zustimmung', 'zusammen', 'zuordnung', 'zugrunde', 'zugang', 'zudem', 'ziel', 'zeitlich', 'zeitgesteuerter', 'zeile', 'zeigen', 'zahlen', 'zahl', 'z.b.:1', 'wpdb', 'wort\\\\', 'wort-tokenisierung', 'wordpress-installation', 'wordpress', 'wohnung', 'wissen', 'windowsto', 'wieviele', 'wiederum', 'werkzeuge', 'werden.das', 'wenn-dann-beziehung', 'wenn-dann-anweisung', 'welt', 'weiteres', 'weitere', 'weisen', 'wegen', 'webseiteninhalte', 'webentwicklern', 'webcrawling', 'webcrawler', 'warum', 'wartungsfreundlichkeit', 'wann', 'wahrscheinlich', 'wahlwerbung', 'wahl', 'wachsend', 'wachsen', 'völlig', 'vorzugsweise', 'vorteil', 'vorne', 'vorliebe', 'vorkommt.tf-idf', 'vorherzusagend', 'vorhersage', 'vorbereitend', 'vor.jetzt', 'vielzahl', 'verweisen', 'verweildauer', 'verstehen', 'verlassen', 'verhalten', 'verglichen:3', 'vergleichbar', 'verfügbar', 'vereinigen', 'vereinfachen.die', 'vereinfachen', 'verarbeitungsgeschwindigkeit', 'verarbeitungsdauer', 'verarbeitungs-', 'verarbeiten', 'veranschaulichung', 'vektorlängen', 'vektor', 'using', 'url', 'untersuchen', 'unterscheiden', 'unternehmen', 'unordered', 'ungeordneten', 'unbrauchbar', 'unbedingt', 'unbearbeitet', 'umwandeln', 'umstritten', 'umstand', 'umreißen', 'umgangssprachlich', 'umgang', 'ul', 'txt', 'trennen', 'trend', 'trainingsprozess', 'training', 'towards', 'toolkit', 'tief', 'the', 'termhäufigkeit', 'tensorflowjava', 'tensorflow.js', 'teilbereich', 'teil', 'technik', 'teachable', 'tausend', 'tatsächlich', 'task', 'tag', 'tabelle', 'syntax', 'synaptisch', 'synapse', 'symptom', 'sv', 'suppe', 'suchfunktion', 'such-vektor', 'such-bag', 'subsymbolisch', 'strategie', 'strartseit', 'stop-wörter', 'stellen', 'stelle', 'statt', 'statistisch', 'stammform:3', 'sql-datenbankabfragen', 'spracherkennung', 'spiegeln', 'spiegel-mining', 'spezifizieren', 'speicherung', 'spalt', 'sowohl', 'sowie', 'source-bibliothek', 'soup', 'sortieren', 'sorgfältig', 'sog', 'softwaresammlung', 'software-paketmanagement', 'sklearn', 'skalarprodukt', 'sinn', 'similarity', 'siezen', 'sicher', 'services', 'service', 'semantrisquick', 'selbstständig', 'seiten-vektoren', 'seinerseits', 'scrape', 'scissorsdiskutiere', 'science', 'schätzen', 'schwellenpotential', 'schrittweise', 'schriftlich', 'schnittstelle', 'schnee', 'schlussfolgerung', 'schlussendlich', 'schließen', 'scenario', 'sammlung', 'rückschluß', 'rolle', 'rock', 'ressourcensparend', 'resources', 'resourcensparender', 'repräsentieren', 'relativ', 'reihen', 'regelmäßigkeit', 'regelbasierter', 'reduzieren', 'records', 'rechnen', 'rechenleistung', 'rechenkerne', 'rechenaufwand', 'realisieren', 'realisert', 'raumcosine', 'rasperry', 'rasch', 'r.text', 'quelloffene', 'quasi', 'python-entwicklungswerkzeuge', 'prozeß', 'prozessschritte', 'provider', 'prototyping-tool', 'programmpaket', 'programmierungbeim', 'programmatisch', 'programm', 'prognose', 'produkt', 'problem', 'probieren', 'prinzipiell', 'positiv', 'politisch', 'plattform', 'pi', 'personell', 'personalisiert', 'partnerarbeit', 'parallel', 'pap', 'ordnung', 'option', 'open-source-bibliothek', 'open', 'online', 'offiziell', 'obwohl', 'objekteidentifizierung', 'nützlich', 'nvidia', 'nutzung', 'nutzer', 'notwendig', 'notieren', 'notebooks', 'notebook', 'normalerweise', 'nltk', 'nlp-basiert', 'nicht-kommerziell', 'nicht-', 'neurotransmitter', 'netzwerkbelastung', 'nervenverbindung', 'nervensystem', 'nervenfasern', 'nerven-', 'nehmen.python', 'negativ', 'neben', 'najeebnik21/activation-function-in-deep-learning-587e83d5a681', 'nahe', 'nacheinander', 'nachbildung', 'nachbilden', 'müssen.das', 'mündlich', 'mulitpliziert', 'moodle-kurs', 'ml-programmpaket', 'mittels', 'missverständlich', 'misha', 'mining', 'mine', 'mindestanzahl', 'milliarde', 'mikrocontrollern', 'method', 'messen', 'merkmal', 'menge', 'mehrere', 'mehr', 'matrizenrechnung', 'manuell', 'manipulation', 'lösungweg', 'lösungsvorschlag', 'lösung', 'logarithmusfunktion', 'logarithmus', 'log', 'listenpunkt', 'listenelement', 'linker', 'like', 'liegend', 'liefern', 'li', 'letzteres', 'letzter', 'lesbar', 'lernprozess', 'lernendabei', 'lernend', 'leistungsschwach', 'leistungsfähigkeit', 'leicht', 'leerzeich', 'learning-bibliotheken', 'lay', 'lauten', 'laufen', 'lange', 'lang', 'können.dazu', 'können.beispiel', 'kurz', 'kritisch', 'kriesels', 'kriesel', 'kriechen', 'kratzen', 'krankheit', 'kostenlos', 'kopieren', 'kontrollstrukturen', 'kompliziert', 'komplexität', 'komplett', 'kompakt', 'kommunikation', 'knowledge', 'kleinschreibung', 'klassisch', 'klassifizieren', 'klassifikationproblem', 'ki-systeme', 'ki-spiele', 'ki-projekten', 'ki-projekte', 'ki-programmierung', 'ki-programmbibliotheken', 'ki-programmbibliothek', 'ki-modell', 'ki-domainen', 'ki-bibliotheken', 'ki-bibliothek', 'ki-anwendungsgebiete', 'ki-', 'key', 'kennen', 'kdd-prozessdata-mining', 'kaufverhalten', 'jupyter-notebooks', 'jupyter-notebook-anbietern', 'jupyter-notebook', 'jupyter', 'jeweils', 'jedoch', 'java', 'item', 'internetseite', 'internetlinks', 'internetbrowser', 'internet-robots', 'interessieren', 'interaktiv', 'intel', 'inspektion', 'insbesondere', 'inhaltsverzeichnis', 'informationstechnisch', 'informatik', 'indizieren', 'individual', 'index', 'in\\\\', 'implementierungsbeispielen', 'immer', 'im\\\\', 'hören', 'hängen', 'huskys', 'hundert', 'hund', 'html5lib', 'html-tags', 'html-inspektion', 'html-information', 'html-', 'html', 'hinblick', 'hinaus', 'hilfe', 'high-level-tools', 'hierfür', 'hierbei', 'hidden', 'heute', 'hervorragend', 'herausfinden', 'helfen', 'heizen', 'heading', 'headers', 'hashmap', 'hanover-lemmatizer', 'handeln', 'h3-element', 'h3', 'gültig', 'gängig', 'größe', 'grund-', 'grund', 'grenze', 'graphic', 'graben', 'googles', 'gliazell', 'gleich', 'gewiß', 'gewichtet.diese', 'gespeichert', 'gesetzmäßigkeit', 'gescrapte', 'geschrieben:2', 'geschlecht', 'gesamtzahl\\\\', 'gesamtwortzahl\\\\', 'gering', 'genauso', 'genau', 'gekennzeichnet', 'gegensatz', 'gefahr', 'gebrauch', 'förderung', 'fördern', 'fällen', 'früh', 'fortschritt', 'fortran', 'formel', 'format', 'folgendes', 'folgen', 'fließkomma-berechnung', 'finden', 'feueren', 'feuer', 'feststellen', 'fest', 'fertig', 'fehlend', 'fallen', 'fall', 'facebook-seitenaufrufe', 'extern', 'explained', 'expertensystem', 'experimentell', 'evaluieren', 'etwa', 'eröffnet', 'erzeugen', 'erwartet', 'erster', 'ersetzen', 'erregen', 'ermöglichen', 'erlernen', 'erledigen', 'erklären', 'erhalten', 'erfreuen', 'erfassung', 'entwicklungsumgebung', 'entfernt', 'eng', 'ende', 'empfangen', 'element', 'einzahl', 'eintreffen', 'eintrag', 'einplatinen-rechner', 'eingesetzt.gelegentlich', 'eingebettet', 'einführung', 'einführen', 'einflussen', 'ein\\\\', 'ehesten', 'eher', 'egal', 'effektivität', 'effekt', 'edition', 'echt', 'dürfen', 'duzen', 'durchsatz', 'durchgehend', 'draw', 'doquelle', 'do', 'dl4j', 'distribution', 'diskutieren', 'discovery', 'difference', 'diejenigen', 'diagnostizieren', 'developer', 'des\\\\', 'deren', 'der\\\\', 'denen\\\\', 'demokratie', 'demnach', 'definition', 'deeplearning4j', 'debugging', 'davon', 'datumensatz', 'datumenbank', 'datumenanalyse', 'datenvisualisierung', 'datenreihen', 'datenmustererkennung', 'datenmerkmalen', 'datenmengen', 'datenexploration', 'datenanalysen', 'datei', 'databases', 'darunter', 'darum', 'darin', 'darauf', 'daran', 'daneben', 'danach', 'd.h.', 'crawl', 'cpus', 'cpu-leistung', 'cor', 'compute', 'comparisons.argmax', 'communities', 'codebasis', 'codeauszug', 'code', 'client-server-anwendung', 'cli', 'chip', 'central', 'ca', 'c.', 'c', 'by', 'bruch', 'botenstoff', 'biologisch', 'binäres', 'binären', 'billiarde', 'bilderkennung', 'bild', 'bias', 'bezeichnet.die', 'bezeichnen.data-mining', 'bevorzugen', 'bevor', 'between', 'betrieb', 'bestmöglich', 'bestimmtes\\\\', 'besonderer', 'besitz', 'beschleunigen', 'besagen', 'bereit', 'benutzeroberfläche', 'benutzer-', 'benutzer', 'beliebtheit', 'beliebt', 'bekannte', 'beispiesweise', 'beispielhaft', 'beim', 'beides', 'befinden', 'befehl', 'befassen', 'bedeutend', 'beautifulsoup', 'beautiful', 'bearbeiten', 'basieren', 'bag_o', 'b_i^2', 'b_i', 'b_2^2+', 'b_1^2', 'automatisierung', 'automatisiert', 'auswahl', 'ausschließen', 'aussagen', 'aussagekräftiger.die', 'auslegen', 'ausgehen', 'ausführung', 'ausführen', 'aufsummierung', 'aufsummieren', 'aufgerufen', 'aufbereitung', 'aufarbeitung', 'asics', 'anwesenheitsdatum', 'anwendungsspezifisch', 'ansprache', 'ansatz', 'anpassungsfähig', 'angesichts', 'aneinander', 'and', 'alternative', 'alter', 'allgemein', 'aller\\\\', 'allein', 'algorithmus', 'aktiv', 'ai', 'abweichung', 'absteigend', 'abspeichern', 'absolut', 'abschwächen', 'abschließend', 'abschalten', 'abkürzung', 'abhängigkeit', 'abgeschnitten', 'abgelegt.jetzt', 'abgeben', 'abbilden', 'abbau', 'ab.den', 'ab', 'a_i^2', 'a_i', 'a_2^2+', 'a_1b_1+a_2b_2+', 'a_1^2', '@', '=cos']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function bagofwords with the following input: page and words\n",
        "def bagofwords(data, vocab):\n",
        "    # frequency word count\n",
        "    bag = np.zeros(len(vocab)) #create a NumPy array made up of zeroes with size len(words)\n",
        "    # loop through data and add value of 1 when token is present in the tweet\n",
        "    for sw in data:\n",
        "        for i,word in enumerate(vocab):\n",
        "            if word == sw: \n",
        "                bag[i] += 1\n",
        "                \n",
        "    return np.array(bag) # return the bag of word for one page"
      ],
      "metadata": {
        "id": "kus5yifih-9q"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = ['künstlich', 'intelligenz', 'maschinell']\n",
        "bagofwords(test, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T8b-f4UjonJ",
        "outputId": "d62a4a73-d4a3-4201-d9f5-51ab3f5c4f58"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "Zt_jEv0nKFtB"
      },
      "outputs": [],
      "source": [
        "# set up a NumPy array with the specified dimension to contain the bag of words\n",
        "n_words = len(vocab)\n",
        "n_docs = len(data)\n",
        "bag_o = np.zeros([n_docs,n_words])\n",
        "# use loop function to add new row for each data of page. \n",
        "for ii in range(n_docs): \n",
        "    #call out the previous function 'bagofwords'. see the inputs: sentence and words\n",
        "    bag_o[ii,:] = bagofwords(data[ii][1], vocab) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bag_o.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZh_YmARpKw-",
        "outputId": "7d7354fd-382f-40de-ea36-be3458cfc0cc"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inverse document frequency"
      ],
      "metadata": {
        "id": "pZHlKeXWt1Ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize 2 variables representing the number of pages (numdocs) and the number of tokens/words (numwords)\n",
        "numdocs, numwords = np.shape(bag_o)\n",
        "\n",
        "#Changing into the tfidf formula as above\n",
        "N = numdocs\n",
        "term_frequency = np.empty(numwords)\n",
        "\n",
        "#Count the number of documents the word appears in.\n",
        "for word in range(numwords):\n",
        "    term_frequency[word]=np.sum(bag_o[:,word]>0) \n",
        "print(term_frequency)\n",
        "idf = np.log(N/term_frequency)\n",
        "print(idf)"
      ],
      "metadata": {
        "id": "zZfFS_vatzp-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0191abcc-fd3d-43e7-ac92-cbaa5644deff"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1. 3. 1. 1. 5. 6. 1. 5. 6. 3. 1. 1. 1. 1. 3. 3. 1. 1. 3. 5. 4. 5.\n",
            " 4. 2. 5. 6. 2. 3. 3. 3. 1. 1. 1. 5. 1. 1. 3. 2. 5. 5. 3. 2. 1. 3. 1. 3.\n",
            " 2. 2. 3. 1. 1. 2. 1. 4. 1. 3. 3. 3. 3. 2. 2. 2. 2. 2. 3. 2. 2. 1. 1. 1.\n",
            " 3. 2. 1. 3. 3. 1. 4. 2. 2. 3. 2. 1. 4. 1. 2. 2. 2. 2. 1. 4. 2. 2. 2. 2.\n",
            " 2. 2. 1. 2. 1. 1. 2. 2. 4. 2. 4. 3. 3. 1. 1. 3. 1. 3. 1. 3. 3. 2. 1. 3.\n",
            " 1. 1. 1. 1. 1. 1. 2. 2. 2. 1. 1. 1. 1. 1. 1. 1. 2. 2. 3. 2. 1. 2. 2. 2.\n",
            " 1. 2. 1. 2. 3. 3. 1. 2. 1. 2. 1. 3. 2. 2. 1. 2. 2. 3. 1. 2. 3. 3. 1. 2.\n",
            " 1. 3. 3. 3. 1. 3. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 1. 1. 1. 2. 2.\n",
            " 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2. 2. 2. 2. 1. 2. 1. 1. 2. 1. 1. 1.\n",
            " 1. 2. 1. 1. 1. 2. 2. 2. 1. 2. 1. 1. 2. 1. 1. 1. 1. 2. 1. 1. 2. 1. 2. 1.\n",
            " 1. 1. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 1. 2. 1. 1. 2. 2. 2. 1. 1. 1. 1. 2.\n",
            " 2. 2. 2. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2. 2.\n",
            " 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 2. 1. 2. 2. 1. 1. 1. 2. 2. 1. 1. 1. 2. 2.\n",
            " 2. 1. 1. 1. 1. 2. 1. 2. 1. 2. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "[3.13549422 3.13549422 3.13549422 2.03688193 3.13549422 3.13549422\n",
            " 1.5260563  1.34373475 3.13549422 1.5260563  1.34373475 2.03688193\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 2.03688193 2.03688193\n",
            " 3.13549422 3.13549422 2.03688193 1.5260563  1.74919985 1.5260563\n",
            " 1.74919985 2.44234704 1.5260563  1.34373475 2.44234704 2.03688193\n",
            " 2.03688193 2.03688193 3.13549422 3.13549422 3.13549422 1.5260563\n",
            " 3.13549422 3.13549422 2.03688193 2.44234704 1.5260563  1.5260563\n",
            " 2.03688193 2.44234704 3.13549422 2.03688193 3.13549422 2.03688193\n",
            " 2.44234704 2.44234704 2.03688193 3.13549422 3.13549422 2.44234704\n",
            " 3.13549422 1.74919985 3.13549422 2.03688193 2.03688193 2.03688193\n",
            " 2.03688193 2.44234704 2.44234704 2.44234704 2.44234704 2.44234704\n",
            " 2.03688193 2.44234704 2.44234704 3.13549422 3.13549422 3.13549422\n",
            " 2.03688193 2.44234704 3.13549422 2.03688193 2.03688193 3.13549422\n",
            " 1.74919985 2.44234704 2.44234704 2.03688193 2.44234704 3.13549422\n",
            " 1.74919985 3.13549422 2.44234704 2.44234704 2.44234704 2.44234704\n",
            " 3.13549422 1.74919985 2.44234704 2.44234704 2.44234704 2.44234704\n",
            " 2.44234704 2.44234704 3.13549422 2.44234704 3.13549422 3.13549422\n",
            " 2.44234704 2.44234704 1.74919985 2.44234704 1.74919985 2.03688193\n",
            " 2.03688193 3.13549422 3.13549422 2.03688193 3.13549422 2.03688193\n",
            " 3.13549422 2.03688193 2.03688193 2.44234704 3.13549422 2.03688193\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 2.44234704 2.44234704 2.44234704 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 2.44234704 2.44234704\n",
            " 2.03688193 2.44234704 3.13549422 2.44234704 2.44234704 2.44234704\n",
            " 3.13549422 2.44234704 3.13549422 2.44234704 2.03688193 2.03688193\n",
            " 3.13549422 2.44234704 3.13549422 2.44234704 3.13549422 2.03688193\n",
            " 2.44234704 2.44234704 3.13549422 2.44234704 2.44234704 2.03688193\n",
            " 3.13549422 2.44234704 2.03688193 2.03688193 3.13549422 2.44234704\n",
            " 3.13549422 2.03688193 2.03688193 2.03688193 3.13549422 2.03688193\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 2.44234704 3.13549422 3.13549422 3.13549422 2.44234704\n",
            " 2.44234704 3.13549422 3.13549422 3.13549422 2.44234704 2.44234704\n",
            " 3.13549422 2.44234704 2.44234704 2.44234704 2.44234704 2.44234704\n",
            " 3.13549422 3.13549422 2.44234704 3.13549422 3.13549422 2.44234704\n",
            " 2.44234704 2.44234704 2.44234704 2.44234704 3.13549422 2.44234704\n",
            " 3.13549422 3.13549422 2.44234704 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 2.44234704 3.13549422 3.13549422 3.13549422 2.44234704\n",
            " 2.44234704 2.44234704 3.13549422 2.44234704 3.13549422 3.13549422\n",
            " 2.44234704 3.13549422 3.13549422 3.13549422 3.13549422 2.44234704\n",
            " 3.13549422 3.13549422 2.44234704 3.13549422 2.44234704 3.13549422\n",
            " 3.13549422 3.13549422 2.44234704 2.44234704 2.44234704 3.13549422\n",
            " 3.13549422 3.13549422 2.44234704 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 2.44234704 3.13549422 3.13549422 2.44234704 2.44234704\n",
            " 2.44234704 3.13549422 3.13549422 3.13549422 3.13549422 2.44234704\n",
            " 2.44234704 2.44234704 2.44234704 3.13549422 2.44234704 2.44234704\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 2.44234704 3.13549422 3.13549422\n",
            " 3.13549422 2.44234704 2.44234704 2.44234704 2.44234704 2.44234704\n",
            " 2.44234704 3.13549422 2.44234704 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 2.44234704 2.44234704 2.44234704 2.44234704 3.13549422\n",
            " 2.44234704 2.44234704 3.13549422 3.13549422 3.13549422 2.44234704\n",
            " 2.44234704 3.13549422 3.13549422 3.13549422 2.44234704 2.44234704\n",
            " 2.44234704 3.13549422 3.13549422 3.13549422 3.13549422 2.44234704\n",
            " 3.13549422 2.44234704 3.13549422 2.44234704 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 2.44234704 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422 3.13549422\n",
            " 3.13549422 3.13549422 3.13549422 3.13549422]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#initializs tfidf array\n",
        "tfidf = np.empty([numdocs, numwords])\n",
        "\n",
        "#loop through the pages, multiply term frequency (represented by bag of words) with idf\n",
        "for doc in range(numdocs):\n",
        "    tfidf[doc, :]=bag_o[doc, :]*idf"
      ],
      "metadata": {
        "id": "lvNFYPM0XVIa"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hw4X28HLunqf",
        "outputId": "6bf12448-ab91-4ec6-ed26-cb159a2faf85"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (tfidf)"
      ],
      "metadata": {
        "id": "psVdAxNMwESb",
        "outputId": "865a8c1d-80e9-477f-c3a5-27f774de54cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This data can be saved so that it does not have to be determined each time with web crawling and NLP (could be done as a task once a day)."
      ],
      "metadata": {
        "id": "bwDDac3d6GNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"tfidf.npy\"   # file extension has to be \"npy\"\n",
        "np.save(filename,tfidf)  # numpy provides file functions"
      ],
      "metadata": {
        "id": "_5Shtx-I6ojA"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data can be loaded by this:"
      ],
      "metadata": {
        "id": "QV68a3IRAkBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf=np.load(filename)"
      ],
      "metadata": {
        "id": "Il36TN8iAYqO"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# search string\n",
        "search='Anaconda installieren maschinell maschinell künstlich'\n",
        "\n",
        "processed = processText(search)\n",
        "print(processed)\n",
        "search_vector = bagofwords(processed, vocab)\n",
        "print(search_vector)\n",
        "\n",
        "#calculate tfidf \n",
        "term_frequency = np.empty(numwords)\n",
        "\n",
        "#Count the number of documents the search word appears in.\n",
        "for word in range(numwords):\n",
        "    term_frequency[word]=np.sum(search_vector[word]>0) \n",
        "print(term_frequency)\n",
        "\n",
        "#initializs tfidf array\n",
        "search_tfidf = np.empty([numwords])\n",
        "\n",
        "#multiply term frequency (represented by bag of words) with idf\n",
        "search_tfidf = term_frequency * idf\n",
        "\n",
        "print(search_tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwMQCpAq02ZL",
        "outputId": "24bb09a6-3c72-4526-9be0-41d68204375c"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['anaconda', 'installieren', 'maschinell', 'maschinell', 'künstlich']\n",
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         1.34373475 0.         1.5260563  0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 3.13549422 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#comparision with search vector without tfidf\n",
        "comparisons = cosine_similarity(tfidf, search_vector.reshape(1,-1))\n",
        "print(comparisons)\n",
        "#comparision with tfdif search vector => better results\n",
        "comparisons = cosine_similarity(tfidf, search_tfidf.reshape(1,-1))\n",
        "print(comparisons)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6m3hCEhvIjh0",
        "outputId": "575a8aaf-4298-418b-8512-1da1aa6b2a69"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.        ]\n",
            " [0.        ]\n",
            " [0.27134659]\n",
            " [0.12493221]\n",
            " [0.02837893]\n",
            " [0.        ]\n",
            " [0.07761732]\n",
            " [0.        ]\n",
            " [0.02166404]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.00836841]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.05799279]\n",
            " [0.19427055]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.07726471]\n",
            " [0.        ]\n",
            " [0.        ]]\n",
            "[[0.        ]\n",
            " [0.        ]\n",
            " [0.16728019]\n",
            " [0.11003497]\n",
            " [0.02499495]\n",
            " [0.        ]\n",
            " [0.05265195]\n",
            " [0.        ]\n",
            " [0.01908076]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.00515897]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.02900396]\n",
            " [0.39925996]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.        ]\n",
            " [0.03864244]\n",
            " [0.        ]\n",
            " [0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# best result\n",
        "print(data[comparisons.argmax()][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1qKH9vXJW8u",
        "outputId": "5507c2d5-cb95-4117-b880-0404bc3eee75"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://eick-at.de/moodle/mod/page/view.php?id=1369\n"
          ]
        }
      ]
    }
  ]
}